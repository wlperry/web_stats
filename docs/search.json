[
  {
    "objectID": "09_ggplot_multiple_plots_patchwork.html",
    "href": "09_ggplot_multiple_plots_patchwork.html",
    "title": "ggplot2 and patchwork - multiple panes",
    "section": "",
    "text": "The goal of this slide set is to show you how to make publication ready graphs and expand on what we have done in the basic plotting series. The goal of this is to use GGPlot and later on I will show you how to use patchwork which allows more flexibility in plot layout."
  },
  {
    "objectID": "09_ggplot_multiple_plots_patchwork.html#plot-grass-lake",
    "href": "09_ggplot_multiple_plots_patchwork.html#plot-grass-lake",
    "title": "ggplot2 and patchwork - multiple panes",
    "section": "Plot Grass Lake",
    "text": "Plot Grass Lake\n\ngrass.plot &lt;- lakes.df %&gt;%\n  filter(lake_name==\"Grass\" & str_detect(genus_species, \"Daphnia\")) %&gt;% \n  ggplot(aes(date, org_l, color=genus_species)) + # sometimes necessary is , group = group\n  geom_point()+\n  geom_line() +\n  labs(x = \"Date\", y = expression(bold(\"Animals (No. L\"^-1*\")\"))) +\n  scale_x_date(date_breaks = \"6 month\",\n               limits = as_date(c('1994-06-01', '2006-12-31')),\n               labels=date_format(\"%Y-%m-%d\"), expand=c(0,0))  +\n  theme(axis.text.x = element_text(size=12, face=\"bold\", angle=45, hjust=1)) \ngrass.plot\n\n\n\n\n\n\n\n\n\n# plot of Indian Lake ------\nindian.plot &lt;- lakes.df %&gt;%\n  filter(lake_name==\"Indian\" & str_detect(genus_species, \"Daphnia\")) %&gt;% \n  ggplot(aes(date, org_l, color=genus_species)) + # sometimes necessary is , group = group\n  geom_point()+\n  geom_line() +\n  labs(x = \"Date\", y = expression(bold(\"Animals (No. L\"^-1*\")\"))) +\n  scale_x_date(date_breaks = \"6 month\",\n               limits = as_date(c('1994-06-01', '2006-12-31')),\n               labels=date_format(\"%Y-%m-%d\"), expand=c(0,0))  +\n  theme(axis.text.x = element_text(size=12, face=\"bold\", angle=45, hjust=1)) \nindian.plot\n\n\n\n\n\n\n\n\n\n# Now we can use patchwork to combine files\n# Lets look at the plots in one format\ngrass.plot +\nwillis.plot +\nindian.plot +\nplot_layout(ncol = 1)\n\n\n\n\n\n\n\n\n\n# here is another format\ngrass.plot +\nwillis.plot +\nindian.plot +\nplot_layout(ncol = 1,\n              heights=c(4,2,1))\n\n\n\n\n\n\n\n\n\n# you might want to turn off legends and axes\n# you can do this in the theme statements\ngrass.plot + theme(axis.title.x=element_blank(), axis.text.x=element_blank(),\n                   legend.position = \"none\") + \nwillis.plot + theme(axis.title.x=element_blank(), axis.text.x=element_blank(),\n                    legend.position = \"none\") +\nindian.plot + theme(axis.title.x=element_blank(), axis.text.x=element_blank(),\n                    legend.position = \"bottom\") +\nplot_layout(ncol = 1,\n            heights=c(4,2,1))\n\n\n\n\n\n\n\n\n\n# the plots can get as fancy as you would like\n\ngrass.plot +  theme(legend.position = \"bottom\") + {\n  willis.plot + theme(axis.title.x=element_blank(), axis.text.x=element_blank(),\n                      legend.position = \"none\") +\n  indian.plot +  theme(legend.position = \"none\") +\n    plot_layout(ncol=1) } +\n  plot_layout(ncol = 2)\n\n\n\n\n\n\n\n\n\n# You can also do this without brackets\n(grass.plot  + theme(axis.title.x=element_blank(), axis.text.x=element_blank(),\n                     legend.position = \"none\") |\nwillis.plot + theme(axis.title.x=element_blank(), axis.text.x=element_blank(),\n                      legend.position = \"none\")) /\nindian.plot +   theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "05_reading_files.html",
    "href": "05_reading_files.html",
    "title": "Reading and Writing Data",
    "section": "",
    "text": "Objective\nLearn how to read data from different file types, process it, and then save your results to an output directory. We’ll cover:\n\nCSV files\nExcel files\nTab-delimited files\nSpace-delimited files\n\nFor more sample data files, see the Dataframes page.\n\n\n\nLoad Required Libraries\nWe’ll use tidyverse for CSV and delimited files, and readxl for Excel files.\n\n# Load necessary libraries\nlibrary(janitor)\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n\nReading Data Files\n\nCSV Files\n\n\n# Read a CSV file\nmm_df &lt;- read_csv(\"data/mms.csv\")\n\n\nExcel Files\n\n\n# Read an Excel file\nmm_excel_df &lt;- read_excel(\"data/mms.xlsx\")\n\n\nTab-Delimited Files\n\n\n# Read a tab-delimited file (alternatively, use read_tsv)\nmm_tab_df &lt;- read_delim(\"data/mms_tab.txt\", delim = \"\\t\")\n\n\nSpace-Delimited Files\n\n\n# Read a space-delimited file\nmm_space_df &lt;- read_delim(\"data/mms_space.txt\", delim = \" \")\n\n\n\nInspecting the Data\nAfter reading in a file, check its structure using:\n\n# Quickly inspect the data\nglimpse(mm_df)\n\nRows: 816\nColumns: 4\n$ center   &lt;chr&gt; \"peanut butter\", \"peanut butter\", \"peanut butter\", \"peanut bu…\n$ color    &lt;chr&gt; \"blue\", \"brown\", \"orange\", \"brown\", \"yellow\", \"brown\", \"yello…\n$ diameter &lt;dbl&gt; 16.20, 16.50, 15.48, 16.32, 15.59, 17.43, 15.45, 17.30, 16.37…\n$ mass     &lt;dbl&gt; 2.18, 2.01, 1.78, 1.98, 1.62, 2.59, 1.90, 2.55, 2.07, 2.26, 1…\n\n# or \nhead(mm_df)\n\n# A tibble: 6 × 4\n  center        color  diameter  mass\n  &lt;chr&gt;         &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 peanut butter blue       16.2  2.18\n2 peanut butter brown      16.5  2.01\n3 peanut butter orange     15.5  1.78\n4 peanut butter brown      16.3  1.98\n5 peanut butter yellow     15.6  1.62\n6 peanut butter brown      17.4  2.59\n\n\n\n\nSaving Processed Data\nBefore saving your results, ensure the output directory exists. You can create it if needed:\n\n# Create the output directory if it doesn't exist\nif (!dir.exists(\"output\")) {\n  dir.create(\"output\")\n}\n\nThen, save your data frame as a CSV file:\n\n# Save the processed data to the output directory\nwrite_csv(mm_df, \"output/mm_output.csv\")\n\n\n\nCleaning up messy or poorly formatted variable names\nTo do this we will use janitor that has a lot of functions that automate this task.\n\n# Read an Excel file\nmm_excel_df &lt;- read_excel(\"data/mms.xlsx\") %&gt;%    \n              clean_names()"
  },
  {
    "objectID": "03_installing_libraries.html",
    "href": "03_installing_libraries.html",
    "title": "Installing Libraries",
    "section": "",
    "text": "Libraries (or packages) are collections of R functions and data that extend R’s capabilities. In this guide, you’ll learn how to install and load some essential libraries to help you get started.\n\n\n\nTo add new functionality to R, you first need to install the libraries. Run the following commands in your R console:\ninstall.packages(\"devtools\")    # Tools for package development\ninstall.packages(\"tidyverse\")     # A suite of packages for data manipulation and visualization\ninstall.packages(\"lubridate\")     # Simplifies working with dates and times\ninstall.packages(\"readxl\")        # Reads Excel files\ninstall.packages(\"janitor\")       # Cleans up data imports\ninstall.packages(\"patchwork\")     # Combines multiple plots\ninstall.packages(\"skimr\")         # Provides quick summary statistics\ninstall.packages(\"plotly\")        # Enhances ggplot2 with interactivity\ninstall.packages(\"scales\")        # Helps with ggplot2 axis scaling```\n\n\nAfter installation, load the libraries at the beginning of all of your R scripts using:\nlibrary(tidyverse) \nlibrary(lubridate) \nlibrary(scales) \nlibrary(readxl) \nlibrary(skimr) \nlibrary(janitor) \nlibrary(patchwork)\n\n\n\n\nSome libraries add helpful tools to RStudio’s Addins menu (but not Positron), making it easier to reformat your code. Install these once and then load them as needed:\ninstall.packages(\"ggThemeAssist\")  # Assists with reformatting code\n\ninstall.packages(\"styler\")         # Automatically styles your code\nAnd load them with:\nlibrary(ggThemeAssist)\nlibrary(styler)\nIn RStudio, you might need to enable the add-ins toolbar. Simply go to View &gt; Show Toolbar to access them.\n\n\n\nAddins menu\n\n\n\n\nThese are some of the better vetted statistical packages in my opinion for Anova and Linear Models\ninstall.packages(\"car\") # stats and ANOVA - essential \ninstall.packages(\"emmeans\") # estimated marginal means for unbalanced designs \n\n# these only have to be installed and not run...\ninstall.packages(\"multcomView\") # paired comparisons - note this will interfear with DPLYR!!\ninstall.packages(\"Rmisc\") # stats \ninstall.packages(\"Hmisc\") # stats install.packages(\"broom\") # output models cleanly \nAnd load them with:\nlibrary(car)\nlibrary(emmeans)"
  },
  {
    "objectID": "03_installing_libraries.html#installing-essential-libraries",
    "href": "03_installing_libraries.html#installing-essential-libraries",
    "title": "Installing Libraries",
    "section": "",
    "text": "To add new functionality to R, you first need to install the libraries. Run the following commands in your R console:\ninstall.packages(\"devtools\")    # Tools for package development\ninstall.packages(\"tidyverse\")     # A suite of packages for data manipulation and visualization\ninstall.packages(\"lubridate\")     # Simplifies working with dates and times\ninstall.packages(\"readxl\")        # Reads Excel files\ninstall.packages(\"janitor\")       # Cleans up data imports\ninstall.packages(\"patchwork\")     # Combines multiple plots\ninstall.packages(\"skimr\")         # Provides quick summary statistics\ninstall.packages(\"plotly\")        # Enhances ggplot2 with interactivity\ninstall.packages(\"scales\")        # Helps with ggplot2 axis scaling```\n\n\nAfter installation, load the libraries at the beginning of all of your R scripts using:\nlibrary(tidyverse) \nlibrary(lubridate) \nlibrary(scales) \nlibrary(readxl) \nlibrary(skimr) \nlibrary(janitor) \nlibrary(patchwork)"
  },
  {
    "objectID": "03_installing_libraries.html#optional-useful-add-in-libraries",
    "href": "03_installing_libraries.html#optional-useful-add-in-libraries",
    "title": "Installing Libraries",
    "section": "",
    "text": "Some libraries add helpful tools to RStudio’s Addins menu (but not Positron), making it easier to reformat your code. Install these once and then load them as needed:\ninstall.packages(\"ggThemeAssist\")  # Assists with reformatting code\n\ninstall.packages(\"styler\")         # Automatically styles your code\nAnd load them with:\nlibrary(ggThemeAssist)\nlibrary(styler)\nIn RStudio, you might need to enable the add-ins toolbar. Simply go to View &gt; Show Toolbar to access them.\n\n\n\nAddins menu\n\n\n\n\nThese are some of the better vetted statistical packages in my opinion for Anova and Linear Models\ninstall.packages(\"car\") # stats and ANOVA - essential \ninstall.packages(\"emmeans\") # estimated marginal means for unbalanced designs \n\n# these only have to be installed and not run...\ninstall.packages(\"multcomView\") # paired comparisons - note this will interfear with DPLYR!!\ninstall.packages(\"Rmisc\") # stats \ninstall.packages(\"Hmisc\") # stats install.packages(\"broom\") # output models cleanly \nAnd load them with:\nlibrary(car)\nlibrary(emmeans)"
  },
  {
    "objectID": "web_stats.html",
    "href": "web_stats.html",
    "title": "web_stats",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "web_stats.html#quarto",
    "href": "web_stats.html#quarto",
    "title": "web_stats",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "04_project_setup.html",
    "href": "04_project_setup.html",
    "title": "Project Management",
    "section": "",
    "text": "A well-planned project makes data curation and final analysis easier. Here’s a simple guide to help you get started.\n\n\n\nData Source:\nIdentify where your data comes from, its format, and the variables (with clear names and units).\nObjective & Output:\nDecide what you want in the end—whether that’s graphs, summary statistics, or reports.\nWorkflow:\nOutline these steps:\n\nHow frequently data is updated and checked (QA/QC).\nAny transformations or calculations.\nProduce a final, cleaned output without altering your original data.\n\n\n\n\n\nKeep your work tidy with a consistent folder structure. For example:\n\nscripts/ – Your R scripts\ndata/ – Raw, read-only data files\noutput/ – Cleaned data and analysis results\nfigures/ – Graphs and plots\ndocuments/ – Project notes and metadata\n\n\n\n\n\nConsistent Naming:\nUse a controlled vocabulary (e.g., snake_case) to avoid spaces and special characters in variable names.\nFormat Awareness:\nMake sure each data column holds the same type (numeric, character, date, etc.) and consider converting wide data to long format for easier analysis.\nDocumentation:\nComment your code using # and maintain a metadata file that explains variable names, units, and any transformations applied.\n\n\n\n\n\nSet Up a Project in RStudio:\nUse RStudio’s project feature to create a new directory with your folders already set up. This ensures all file paths are relative and consistent across different systems.\n\nBy following these simple steps, you’ll build a solid foundation for curating your data and conducting your final analysis.\n\n\n\nIt is very helpful to use the same design for each project to organize the files. I typically use\n\ndata\nscripts\noutput\nfigures\ndocuments"
  },
  {
    "objectID": "04_project_setup.html#plan-your-data-flow",
    "href": "04_project_setup.html#plan-your-data-flow",
    "title": "Project Management",
    "section": "",
    "text": "Data Source:\nIdentify where your data comes from, its format, and the variables (with clear names and units).\nObjective & Output:\nDecide what you want in the end—whether that’s graphs, summary statistics, or reports.\nWorkflow:\nOutline these steps:\n\nHow frequently data is updated and checked (QA/QC).\nAny transformations or calculations.\nProduce a final, cleaned output without altering your original data."
  },
  {
    "objectID": "04_project_setup.html#organize-your-project-structure",
    "href": "04_project_setup.html#organize-your-project-structure",
    "title": "Project Management",
    "section": "",
    "text": "Keep your work tidy with a consistent folder structure. For example:\n\nscripts/ – Your R scripts\ndata/ – Raw, read-only data files\noutput/ – Cleaned data and analysis results\nfigures/ – Graphs and plots\ndocuments/ – Project notes and metadata"
  },
  {
    "objectID": "04_project_setup.html#standardize-and-document-your-data",
    "href": "04_project_setup.html#standardize-and-document-your-data",
    "title": "Project Management",
    "section": "",
    "text": "Consistent Naming:\nUse a controlled vocabulary (e.g., snake_case) to avoid spaces and special characters in variable names.\nFormat Awareness:\nMake sure each data column holds the same type (numeric, character, date, etc.) and consider converting wide data to long format for easier analysis.\nDocumentation:\nComment your code using # and maintain a metadata file that explains variable names, units, and any transformations applied."
  },
  {
    "objectID": "04_project_setup.html#get-started-with-r-and-rstudio",
    "href": "04_project_setup.html#get-started-with-r-and-rstudio",
    "title": "Project Management",
    "section": "",
    "text": "Set Up a Project in RStudio:\nUse RStudio’s project feature to create a new directory with your folders already set up. This ensures all file paths are relative and consistent across different systems.\n\nBy following these simple steps, you’ll build a solid foundation for curating your data and conducting your final analysis."
  },
  {
    "objectID": "04_project_setup.html#designing-the-folder-structure-of-each-project",
    "href": "04_project_setup.html#designing-the-folder-structure-of-each-project",
    "title": "Project Management",
    "section": "",
    "text": "It is very helpful to use the same design for each project to organize the files. I typically use\n\ndata\nscripts\noutput\nfigures\ndocuments"
  },
  {
    "objectID": "13_joining_data_frames.html",
    "href": "13_joining_data_frames.html",
    "title": "A brief introduction to joining dataframes",
    "section": "",
    "text": "The goal of this page is to learn to join dataframes."
  },
  {
    "objectID": "13_joining_data_frames.html#what-is-datetime-really---when-did-time-begin",
    "href": "13_joining_data_frames.html#what-is-datetime-really---when-did-time-begin",
    "title": "A brief introduction to joining dataframes",
    "section": "What is datetime really - When did Time begin?",
    "text": "What is datetime really - When did Time begin?\nIn R date time like in UNIX is the nubmber of seconds since 1970-01-01 00:00:00 and that will comme in handy in a few minutes.\n\n# What do you think we would do for the date column? \n# Modify the code below\nexo.df &lt;- exo.df %&gt;% \n  mutate(date = (date))"
  },
  {
    "objectID": "06_ggplot.html",
    "href": "06_ggplot.html",
    "title": "Plotting with ggplot2",
    "section": "",
    "text": "In this tutorial you will learn how to: - Read data from an Excel file.\n- Create basic plots using ggplot2.\n- Layer multiple geoms and add custom axis labels.\nFor more sample data files, check out the Data Files page."
  },
  {
    "objectID": "06_ggplot.html#basic-scatter-plot",
    "href": "06_ggplot.html#basic-scatter-plot",
    "title": "Plotting with ggplot2",
    "section": "Basic Scatter Plot",
    "text": "Basic Scatter Plot\nThis example creates a simple scatter plot showing the relationship between candy color and diameter.\n\n# Create a scatter plot:\n# - data = mm_df: specifies the data frame to use.\n# - aes(x = color, y = diameter): maps the 'color' variable to the x-axis and 'diameter' to the y-axis.\n# - geom_point(): adds points for each observation.\nggplot(data = mm_df, aes(x = color, y = diameter)) +\n  geom_point()"
  },
  {
    "objectID": "06_ggplot.html#adding-layers",
    "href": "06_ggplot.html#adding-layers",
    "title": "Plotting with ggplot2",
    "section": "Adding Layers",
    "text": "Adding Layers\nYou can combine multiple geoms to enrich your plot. Here, we add a boxplot behind the points.\n\nggplot(mm_df, aes(x = color, y = diameter)) +\n  geom_boxplot(fill = \"blue\") +  # Adds a boxplot with blue fill for each candy color group.\n  geom_point()                   # Overlays the scatter plot on top."
  },
  {
    "objectID": "06_ggplot.html#adding-axis-labels",
    "href": "06_ggplot.html#adding-axis-labels",
    "title": "Plotting with ggplot2",
    "section": "Adding Axis Labels",
    "text": "Adding Axis Labels\nCustom axis labels help explain what your plot shows. Use the labs() function to add plain text labels.\n\nggplot(mm_df, aes(x = color, y = diameter)) +\n  geom_boxplot(fill = \"blue\") +\n  geom_point() +\n  labs(\n    x = \"Candy Color\",\n    y = \"Candy Diameter (mm)\"\n  )"
  },
  {
    "objectID": "06_ggplot.html#formatted-axis-labels",
    "href": "06_ggplot.html#formatted-axis-labels",
    "title": "Plotting with ggplot2",
    "section": "Formatted Axis Labels",
    "text": "Formatted Axis Labels\nFor more advanced labeling, you can use expressions to format text. In the example below, the y-axis label is bold and includes the Greek letter µ.\n\nggplot(mm_df, aes(x = color, y = diameter)) +\n  geom_boxplot() +\n  geom_point() +\n  labs(\n    x = \"Color\",\n    y = expression(bold(\"Diameter (\" * mu * \"m)\")) # it is als a code \"\\u00b5\"\n  )"
  },
  {
    "objectID": "10_manipulating_select_filter.html",
    "href": "10_manipulating_select_filter.html",
    "title": "Modifying dataframes - select and filter",
    "section": "",
    "text": "The goal of this page is to learn to manipulate dataframes."
  },
  {
    "objectID": "10_manipulating_select_filter.html#filtering-data",
    "href": "10_manipulating_select_filter.html#filtering-data",
    "title": "Modifying dataframes - select and filter",
    "section": "Filtering data",
    "text": "Filtering data\nWe just saw how to reorder columns or remove them. Here we will go over how to filter data to remove rows based on different statements.\n\n# Filtering data and counting data ----\n# there are several boolean operators that are useful for filtering data\n# we can use these to just see the data or we can use to \n\n# lets say we wanted to look at only one lake\nlakes.df %&gt;% filter(org_l &gt;5) %&gt;%  filter(lake_name == \"Willis\")\n\n# A tibble: 32 × 5\n   group      genus_species          date       lake_name org_l\n   &lt;chr&gt;      &lt;chr&gt;                  &lt;date&gt;     &lt;chr&gt;     &lt;dbl&gt;\n 1 Cladoceran Holopedium_giberum     1996-06-17 Willis    31.6 \n 2 Copepod    Leptodiaptomus_minutus 1996-06-17 Willis    19.6 \n 3 Cladoceran Bosmina_longirostris   1998-07-13 Willis     7.96\n 4 Cladoceran Bosmina_longirostris   2001-07-17 Willis     5.19\n 5 Cladoceran Bosmina_longirostris   2006-07-18 Willis     7.5 \n 6 Cladoceran Holopedium_giberum     2006-07-18 Willis     5.15\n 7 Copepod    Leptodiaptomus_minutus 2006-07-18 Willis    17.2 \n 8 Copepod    Leptodiaptomus_minutus 2005-07-19 Willis    11   \n 9 Copepod    Leptodiaptomus_minutus 1996-07-22 Willis    18.2 \n10 Copepod    Mesocyclops_edax       1994-07-27 Willis    33.1 \n# ℹ 22 more rows\n\n\nthese could also be combined"
  },
  {
    "objectID": "10_manipulating_select_filter.html#count",
    "href": "10_manipulating_select_filter.html#count",
    "title": "Modifying dataframes - select and filter",
    "section": "Count",
    "text": "Count\nThe count statement lets us explore parts of the data and see what the data looks like.\n\n# lets look at some of the data using some simple methods\n# how many lakes are there and how many \nlakes.df %&gt;% count(lake_name)\n\n# A tibble: 4 × 2\n  lake_name     n\n  &lt;chr&gt;     &lt;int&gt;\n1 Grass       348\n2 Indian      348\n3 South       348\n4 Willis      324\n\n\n\n# lets see how many genus species there are\nlakes.df %&gt;% count(genus_species)\n\n# A tibble: 12 × 2\n   genus_species               n\n   &lt;chr&gt;                   &lt;int&gt;\n 1 Aglaodiaptomus_leptpus    114\n 2 Bosmina_longirostris      114\n 3 Cyclops_scutifer          114\n 4 Daphnia_catawba           114\n 5 Daphnia_pulex             114\n 6 Diaphanosoma_birgei       114\n 7 Diaphanosoma_brachyurum   114\n 8 Epischura_lacustris       114\n 9 Holopedium_giberum        114\n10 Leptodiaptomus_minutus    114\n11 Mesocyclops_edax          114\n12 Tropocyclops_extensus     114\n\n\n\n# now this is odd all have the same N \n# lets look at what the data is a bit more\nlakes.df %&gt;% \n  group_by(genus_species) %&gt;%\n  filter(org_l==0) %&gt;%\n  count(genus_species)\n\n# A tibble: 12 × 2\n# Groups:   genus_species [12]\n   genus_species               n\n   &lt;chr&gt;                   &lt;int&gt;\n 1 Aglaodiaptomus_leptpus     87\n 2 Bosmina_longirostris       21\n 3 Cyclops_scutifer          113\n 4 Daphnia_catawba            42\n 5 Daphnia_pulex              88\n 6 Diaphanosoma_birgei       113\n 7 Diaphanosoma_brachyurum    66\n 8 Epischura_lacustris       112\n 9 Holopedium_giberum         55\n10 Leptodiaptomus_minutus      3\n11 Mesocyclops_edax            5\n12 Tropocyclops_extensus      65\n\n\n\n# So there are a lot of 0s - what if we removed that.\nlakes.df %&gt;% \n  filter(org_l != 0) %&gt;%\n  count(genus_species)\n\n# A tibble: 12 × 2\n   genus_species               n\n   &lt;chr&gt;                   &lt;int&gt;\n 1 Aglaodiaptomus_leptpus     27\n 2 Bosmina_longirostris       93\n 3 Cyclops_scutifer            1\n 4 Daphnia_catawba            72\n 5 Daphnia_pulex              26\n 6 Diaphanosoma_birgei         1\n 7 Diaphanosoma_brachyurum    48\n 8 Epischura_lacustris         2\n 9 Holopedium_giberum         59\n10 Leptodiaptomus_minutus    111\n11 Mesocyclops_edax          109\n12 Tropocyclops_extensus      49"
  },
  {
    "objectID": "10_manipulating_select_filter.html#ifelse-and-flagging-of-data",
    "href": "10_manipulating_select_filter.html#ifelse-and-flagging-of-data",
    "title": "Modifying dataframes - select and filter",
    "section": "Ifelse and flagging of data",
    "text": "Ifelse and flagging of data\nYou can use the ifelse command to do a lot of basic flagging and modification of data\n\n# Conditional flagging of outliers\n# if else ----\n# what if we wanted to flag all 0  values\n\nlakes.df &lt;- lakes.df %&gt;%\n  mutate(flag = ifelse(org_l==0, \"ZERO\", \"NOT ZERO\"))"
  },
  {
    "objectID": "10_manipulating_select_filter.html#case_when---if-else-on-steroids",
    "href": "10_manipulating_select_filter.html#case_when---if-else-on-steroids",
    "title": "Modifying dataframes - select and filter",
    "section": "Case_when - if else on steroids",
    "text": "Case_when - if else on steroids\nThe case_when statement allows a lot more flexibility\n\n# case when\n# we can do the same thing with case_when\nlakes.df &lt;- lakes.df %&gt;%\n  mutate(flag = case_when(org_l == 0 ~ \"ZERO\",\n                          org_l &gt;0 & org_l &lt; 10  ~ \"1 to 10\",\n                          org_l &gt;=10 & org_l &lt;100 ~ \"10 to 100\",\n                          TRUE ~ \"something else\"))"
  },
  {
    "objectID": "12_manipulating_factors_forecats.html",
    "href": "12_manipulating_factors_forecats.html",
    "title": "A Brief introduction to factors",
    "section": "",
    "text": "Objective\nThe goal of this page is to learn to work with factors.\n\n\nLoad Libraies\n\n# Load Libraries ----\n# this is done each time you run a script\nlibrary(\"readxl\") # read in excel files\nlibrary(\"tidyverse\") # dplyr and piping and ggplot etc\nlibrary(\"lubridate\") # dates and times\nlibrary(\"scales\") # scales on ggplot ases\nlibrary(\"skimr\") # quick summary stats\nlibrary(\"janitor\") # clean up excel imports\nlibrary(\"patchwork\") # multipanel graphs\n\n\n\nRead in files\n\n# If you were typing in data this might be how it looks\n# Read in wide dataframe ----\nlakes.df &lt;- read_csv(\"data/reduced_lake_long_genus_species.csv\")\n\nFactors are used to help convert a continuous or categorical variable into discrete categorical variables used in statistics\nYou will still see the variable as you normall would Lakes - Willis, Grass, Indian, South\nWhat is really happening - they are alphabetized Indian, Grass, South Willis\nand behind the scenes they are assigned a number 1 - n 1, 2, 3, 4\n\n# Convert to factors -----\nlakes.df &lt;- lakes.df %&gt;%\n  mutate(lake_name = as.factor(lake_name))\n\n\n# look at the levels\nlevels(lakes.df$lake_name)\n\n[1] \"Grass\"  \"Indian\" \"South\"  \"Willis\"\n\n\n\n# What if we wanted them reordered to some particular order\nlakes.df &lt;- lakes.df %&gt;%\n  mutate(\n    lake_name = fct_relevel(lake_name, \n                       \"Willis\", \"South\", \"Indian\", \"Grass\"))\n\n\n# here is an example of where you might want it...\n# we can offset the points so they dont overlap\nlakes.df  %&gt;% ggplot(aes(year, color = lake_name)) + \n  stat_summary(aes(y = org_l),\n               fun.y = mean, na.rm = TRUE,\n               geom = \"point\",\n               size = 3, \n               position = position_dodge(0.2)) +\n  stat_summary(aes(y = org_l),\n               fun.data = mean_se, na.rm = TRUE,\n               geom = \"errorbar\",\n               width = 0.2, \n               position = position_dodge(0.2)) +\n  labs(x = \"Date\", y = \"Animals (Number per Liter)\") +\n  scale_color_manual(name = \"Group\",\n                     values = c(\"Willis\" = \"blue\", \"South\"= \"red\", \n                                \"Indian\"= \"black\", \"Grass\"=\"purple\"),\n                     labels = c(\"Willis\" = \"Willis L.\", \"South\" = \"South L.\", \n                                \"Indian\" = \"Indian L.\", \"Grass\" = \"Grass L.\")) +\n  facet_grid(group ~ lake_name)\n\n\n\n\n\n\n\n\n\n# So what if you wanted to see Cladoceran on the bottom?\nlakes.df &lt;- lakes.df %&gt;%\n  mutate(\n    group = fct_relevel(group, \n                            \"Copepod\", \"Cladoceran\"))\n\n\n# here is an example of where you might want it...\n# we can offset the points so they dont overlap\nlakes.df  %&gt;% ggplot(aes(year, color = lake_name)) + \n  stat_summary(aes(y = org_l),\n               fun.y = mean, na.rm = TRUE,\n               geom = \"point\",\n               size = 3, \n               position = position_dodge(0.2)) +\n  stat_summary(aes(y = org_l),\n               fun.data = mean_se, na.rm = TRUE,\n               geom = \"errorbar\",\n               width = 0.2, \n               position = position_dodge(0.2)) +\n  labs(x = \"Date\", y = \"Animals (Number per Liter)\") +\n  scale_color_manual(name = \"Group\",\n                     values = c(\"Willis\" = \"blue\", \"South\"= \"red\", \n                                \"Indian\"= \"black\", \"Grass\"=\"purple\"),\n                     labels = c(\"Willis\" = \"Willis L.\", \"South\" = \"South L.\", \n                                \"Indian\" = \"Indian L.\", \"Grass\" = \"Grass L.\")) +\n  facet_grid(group ~ lake_name)"
  },
  {
    "objectID": "07_customize_ggplots.html",
    "href": "07_customize_ggplots.html",
    "title": "ggplot2: Customizing Plots",
    "section": "",
    "text": "This guide demonstrates how to customize ggplot2 graphs. You will learn to: - Map additional aesthetics (color, shape) to your data. - Adjust point positions using jittering and dodging. - Manually assign colors with scale_color_manual. - Facet your graphs using facet_wrap() and facet_grid()."
  },
  {
    "objectID": "07_customize_ggplots.html#creating-a-simple-xy-plot",
    "href": "07_customize_ggplots.html#creating-a-simple-xy-plot",
    "title": "ggplot2: Customizing Plots",
    "section": "1. Creating a Simple XY Plot",
    "text": "1. Creating a Simple XY Plot\nStart with a basic scatter plot of color versus mass. The aes() function maps your variables to the x and y axes.\n\nggplot(mm_df, aes(color, mass)) +\n  geom_point()"
  },
  {
    "objectID": "07_customize_ggplots.html#mapping-additional-aesthetics",
    "href": "07_customize_ggplots.html#mapping-additional-aesthetics",
    "title": "ggplot2: Customizing Plots",
    "section": "2. Mapping Additional Aesthetics",
    "text": "2. Mapping Additional Aesthetics\nYou can also map aesthetics like color and shape to your data. In the example below:\n\ncolor = color maps the candy color to the point color.\nshape = center maps the candy center type to the point shape.\n\n\nggplot(mm_df, aes(color, mass, color = color, shape = center)) +\n  geom_point()"
  },
  {
    "objectID": "07_customize_ggplots.html#adjusting-point-positions",
    "href": "07_customize_ggplots.html#adjusting-point-positions",
    "title": "ggplot2: Customizing Plots",
    "section": "3. Adjusting Point Positions",
    "text": "3. Adjusting Point Positions\nTo reduce point overlap, you can adjust positions using:\n\nJitter: Adds random noise to points.\nDodge: Offsets points based on a grouping variable.\nJitter-Dodge: Combines both techniques.\n\n\nExample: Jittering Points\n\nggplot(mm_df, aes(color, mass, color = color, shape = center)) +\n  geom_point(position = position_jitter(width = 0.4))"
  },
  {
    "objectID": "07_customize_ggplots.html#customizing-colors",
    "href": "07_customize_ggplots.html#customizing-colors",
    "title": "ggplot2: Customizing Plots",
    "section": "4. Customizing Colors",
    "text": "4. Customizing Colors\n\nMethod 1: Manual Color Assignment (Order-Dependent)\nThis method sets a palette by order. (Be cautious if the order of factor levels changes.)\n\nggplot(mm_df, aes(color, mass, color = color, shape = center)) +\n  geom_boxplot() +\n  geom_point(position = position_jitterdodge(jitter.width = 0.4)) +\n  labs(x = \"Color\", y = \"Mass\") +\n  scale_color_manual(\n    name = \"Color\",\n    values = c(\"brown\", \"blue\", \"green\", \"orange\", \"red\", \"yellow\"),\n    labels = c(\"Blue\", \"Brown\", \"Green\", \"Orange\", \"Red\", \"Yellow\")\n  )\n\n\n\n\n\n\n\n\n\n\nMethod 2: Safer Manual Color Assignment (1:1 Mapping)\nThis method explicitly assigns colors to each factor level.\n\nggplot(mm_df, aes(color, mass, color = color, shape = center)) +\n  geom_boxplot() +\n  geom_point(position = position_jitterdodge(jitter.width = 0.4)) +\n  labs(x = \"Color\", y = \"Mass\") +\n  scale_color_manual(\n    name = \"Color\",\n    values = c(\n      \"blue\" = \"blue\",\n      \"brown\" = \"brown\",\n      \"green\" = \"green\",\n      \"orange\" = \"orange\",\n      \"red\" = \"red\",\n      \"yellow\" = \"yellow\"\n    ),\n    labels = c(\n      \"blue\" = \"Cool Blue\",\n      \"brown\" = \"Earth Brown\",\n      \"green\" = \"Leaf Green\",\n      \"orange\" = \"Bright Orange\",\n      \"red\" = \"Vivid Red\",\n      \"yellow\" = \"Sunny Yellow\"\n    )\n  )"
  },
  {
    "objectID": "07_customize_ggplots.html#customizing-shapes-of-points",
    "href": "07_customize_ggplots.html#customizing-shapes-of-points",
    "title": "ggplot2: Customizing Plots",
    "section": "5. Customizing shapes of points",
    "text": "5. Customizing shapes of points\n\nMethod 1: Manual shape Assignment (Order-Dependent)\nThis method sets a shape by order. (Be cautious if the order of factor levels changes.)\n\nggplot(mm_df, aes(color, mass, color = color, shape = center)) +\n  geom_boxplot() +\n  geom_point(position = position_jitterdodge(jitter.width = 0.4)) +\n  labs(x = \"Color\", y = \"Mass\") +\n  scale_shape_manual(\n    name = \"Center\",\n    values = c(16, 17, 18, 15, 3, 8),  # shape codes assigned in order\n    labels = c(\"Solid Circle\", \"Triangle\", \"Diamond\", \"Square\", \"Plus\", \"Star\")\n  )\n\n\n\n\n\n\n\n\n\n\nMethod 2: Safer Manual Shape Assignment (1:1 Mapping)\nThis method explicitly maps each factor level (using its name) to a specific shape code. This approach is more robust if the order of the factor levels changes.\n\nggplot(mm_df, aes(color, mass, color = color, shape = center)) +\n  geom_boxplot() +\n  geom_point(position = position_jitterdodge(jitter.width = 0.4)) +\n  labs(x = \"Color\", y = \"Mass\") +\n  scale_shape_manual(\n    name = \"Center\",\n    values = c(\n      \"plain\"  = 16,  # e.g., plain center mapped to a solid circle\n      \"peanut\" = 17,  # peanut center mapped to a triangle\n      \"crispy\" = 18,  # crispy center mapped to a diamond\n      \"wafer\"  = 15,  # wafer center mapped to a square\n      \"malted\" = 3,   # malted center mapped to a plus\n      \"other\"  = 8    # other center mapped to a star\n    ),\n    labels = c(\n      \"plain\"  = \"Plain Center\",\n      \"peanut\" = \"Peanut Center\",\n      \"crispy\" = \"Crispy Center\",\n      \"wafer\"  = \"Wafer Center\",\n      \"malted\" = \"Malted Center\",\n      \"other\"  = \"Other Center\"\n    )\n  )\n\n\n\n\n\n\n\n\nKey Points:\n\n1:1 Mapping: Each factor level in center is explicitly mapped to a specific shape code.\nCustom Labels: The labels argument lets you customize the legend text for clarity."
  },
  {
    "objectID": "11_manipulating_summarizing.html",
    "href": "11_manipulating_summarizing.html",
    "title": "Modifying variables - mutate and summarize",
    "section": "",
    "text": "Objective\nThe goal of this page is to learn to sumarize data and use grouping variables.\n\n\nData for the Exercise\n\n\nA lot of the data was modified from\n\n\nLeach, TH, LA Winslow, FW Acker, JA Bloomfield, CW Boylen, PA Bukaveckas,\n\n\nDF Charles, RA Daniels, CT Driscoll, LW Eichler, JL Farrell, CS Funk,\n\n\nCA Goodrich, TM Michelena, SA Nierzwicki-Bauer, KM Roy, WH Shaw,\n\n\nJW Sutherland, MW Swinton, DA Winkler, KC Rose.\n\n\nLong-term dataset on aquatic responses to concurrent climate change\n\n\nand recovery from acidification. 2018. Scientific Data. online.\n\n\nhttps://doi.org/10.1038/sdata.2018.59. 10.1038/sdata.2018.59\n\n\nLoad Libraries\nAgain, we use these libraries almost all the time in every script\n\n# Load Libraries ----\n# this is done each time you run a script\nlibrary(readxl) # read in excel files\nlibrary(tidyverse) # dplyr and piping and ggplot etc\nlibrary(lubridate) # dates and times\nlibrary(scales) # scales on ggplot ases\nlibrary(skimr) # quick summary stats\nlibrary(janitor) # clean up excel imports\nlibrary(patchwork) # multipanel graphs\n\n\n\nSo now we have seen how to look at the data\n\n\nWhat if we wanted to modify the data in terms of columns or rows\n\n# lets read in a new file to add some complexity for fun\nlakes.df &lt;- read_csv(\"data/reduced_lake_long_genus_species.csv\")\n\n\n\nMutate —–\n\n\nIf you want to modify variables you can change them with MUTATE\n\n# Mutate - log\nlakes_modified.df &lt;- lakes.df %&gt;%\n  mutate(log_org_l = log10(org_l + 1))\n\n\n# Mutate and mean ----\nlakes_modified.df &lt;- lakes.df %&gt;%\n  mutate(mean_org_l = mean(org_l, na.rm=TRUE))\n\n\n# Mean by group ------\nlakes_modified.df &lt;- lakes.df %&gt;%\n  group_by(group) %&gt;%\n  mutate(mean_org_l = mean(org_l, na.rm=TRUE))\n\n\n# how would you modify this to do the mean by group and lake?\nlakes_modified.df &lt;- lakes.df %&gt;%\n  group_by(group) %&gt;%\n  mutate(mean_org_l = mean(org_l, na.rm=TRUE))\n\n\n# Mean and Standard Error -----\n# there is no na.rm=TRUE for sum so we have to do some \n# special things\nlakes_modified.df &lt;- lakes.df %&gt;%\n  group_by(group) %&gt;%\n  mutate(mean_org_l = mean(org_l, na.rm=TRUE),\n         se_org_l = sd(org_l, na.rm = T) / sqrt(sum(!is.na(org_l))))\n\n\n\nSo mutate is a key thing we will use a lot in the future\n\n\nbut this just adds a new column\n\n\nSummarize data —-\n\n\nWhat if we wanted a summary dataset rather than adding a new column\n\n# there are two ways...\n# the first is do all of the math manually\nlakes_summary.df &lt;- lakes.df %&gt;%\n  group_by(lake_name, group) %&gt;%\n  summarize(mean_org_l = mean(org_l, na.rm=TRUE),\n         se_org_l = sd(org_l, na.rm = T) / sqrt(sum(!is.na(org_l))))\n\n\n\nthe other way to do this is using skimr to look at summary data\n\nlakes.df %&gt;% group_by(lake_name, group) %&gt;% skim(org_l)\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n1368\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nlake_name, group\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nlake_name\ngroup\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\norg_l\nGrass\nCladoceran\n0\n1\n1.50\n3.12\n0\n0\n0.08\n1.63\n19.48\n▇▁▁▁▁\n\n\norg_l\nGrass\nCopepod\n0\n1\n4.88\n9.47\n0\n0\n0.00\n4.51\n46.06\n▇▁▁▁▁\n\n\norg_l\nIndian\nCladoceran\n0\n1\n2.58\n7.19\n0\n0\n0.00\n0.80\n56.20\n▇▁▁▁▁\n\n\norg_l\nIndian\nCopepod\n0\n1\n3.21\n6.72\n0\n0\n0.07\n1.96\n34.22\n▇▁▁▁▁\n\n\norg_l\nSouth\nCladoceran\n0\n1\n1.44\n5.00\n0\n0\n0.00\n0.62\n55.60\n▇▁▁▁▁\n\n\norg_l\nSouth\nCopepod\n0\n1\n3.27\n7.76\n0\n0\n0.00\n2.04\n56.03\n▇▁▁▁▁\n\n\norg_l\nWillis\nCladoceran\n0\n1\n1.87\n5.91\n0\n0\n0.00\n1.06\n48.35\n▇▁▁▁▁\n\n\norg_l\nWillis\nCopepod\n0\n1\n2.35\n7.00\n0\n0\n0.00\n0.73\n57.34\n▇▁▁▁▁\n\n\n\n\n\n\n# this can be saved to a dataframe as well\nskim.df &lt;- lakes.df %&gt;% dplyr::group_by(group) %&gt;% skim(org_l)\n\n\n\nthere are a lot of things we can do with mutate and the possibilities are\n\n\nendless. What would you like to see done?"
  },
  {
    "objectID": "08_ggplot_summary_plots.html",
    "href": "08_ggplot_summary_plots.html",
    "title": "ggplot2 Summary Plots: Mean & Standard Error",
    "section": "",
    "text": "In this guide you will learn how to: - Read in data from an Excel file. - Compute summary statistics (mean and standard error). - Create ggplot2 plots that display the mean and standard error using stat_summary()."
  },
  {
    "objectID": "08_ggplot_summary_plots.html#basic-mean-and-se-plot",
    "href": "08_ggplot_summary_plots.html#basic-mean-and-se-plot",
    "title": "ggplot2 Summary Plots: Mean & Standard Error",
    "section": "1. Basic Mean and SE Plot",
    "text": "1. Basic Mean and SE Plot\nThis plot shows the mean diameter for each candy color with standard error error bars.\n\nggplot(mm_df, aes(x = color, y = diameter, color = color)) +\n  stat_summary(fun = mean, na.rm = TRUE, geom = \"point\", size = 3) +\n  stat_summary(fun.data = mean_se, na.rm = TRUE, geom = \"errorbar\", width = 0.2) +\n  labs(\n    x = \"Candy Color\",\n    y = \"Diameter (units)\",\n    title = \"Mean Diameter with Standard Error\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "08_ggplot_summary_plots.html#adding-grouping-by-center",
    "href": "08_ggplot_summary_plots.html#adding-grouping-by-center",
    "title": "ggplot2 Summary Plots: Mean & Standard Error",
    "section": "2. Adding Grouping by Center",
    "text": "2. Adding Grouping by Center\nHere, we add a shape mapping to distinguish between different candy centers (e.g., plain, peanut, etc.).\n\nggplot(mm_df, aes(x = color, y = diameter, color = color, shape = center)) +\n  stat_summary(fun = mean, na.rm = TRUE, geom = \"point\", size = 3) +\n  stat_summary(fun.data = mean_se, na.rm = TRUE, geom = \"errorbar\", width = 0.3) +\n  labs(\n    x = \"Candy Color\",\n    y = \"Diameter (units)\",\n    title = \"Mean Diameter with SE Grouped by Center\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "08_ggplot_summary_plots.html#dodging-for-better-separation",
    "href": "08_ggplot_summary_plots.html#dodging-for-better-separation",
    "title": "ggplot2 Summary Plots: Mean & Standard Error",
    "section": "3. Dodging for Better Separation",
    "text": "3. Dodging for Better Separation\nWhen grouping by center, points and error bars may overlap. Use position_dodge() to separate them.\n\nggplot(mm_df, aes(x = color, y = diameter, color = color, shape = center)) +\n  stat_summary(\n    fun = mean, na.rm = TRUE, geom = \"point\", size = 3,\n    position = position_dodge(width = 0.3)\n  ) +\n  stat_summary(\n    fun.data = mean_se, na.rm = TRUE, geom = \"errorbar\", width = 0.3,\n    position = position_dodge(width = 0.3)\n  ) +\n  labs(\n    x = \"Candy Color\",\n    y = \"Diameter (units)\",\n    title = \"Mean Diameter with SE (Dodged by Center)\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "15_correlations.html",
    "href": "15_correlations.html",
    "title": "Correlations",
    "section": "",
    "text": "The goal of this page is to learn to perform a correlation.\n\n\n##Load libraries We will read in the main files and load the libraries as we have worked with so far.\n\n# One new package for summary stats\n# install.packages(\"broom\")\n# install.packages(\"GGally\")\n# install.packages(\"car\")\n# install.packages(\"gvlma\")\n# install.packages(\"corrplot\")\n# install.packages(\"gvlma\")\n\n# load the libraries each time you restart R\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(skimr)\nlibrary(janitor)\nlibrary(patchwork)\n# library(reshape2)\nlibrary(broom)\nlibrary(GGally)\nlibrary(corrplot)\nlibrary(car)\nlibrary(gvlma)\n\n\n# read in the file\niris.df &lt;- read_csv(\"data/iris.csv\") %&gt;%\n  clean_names()\n\nglimpse(iris.df)\n\nRows: 150\nColumns: 5\n$ sepal_length &lt;dbl&gt; 5.1, NA, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.8…\n$ sepal_width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ petal_length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ petal_width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ species      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa…\n\n\n##Summary Statistics for the better look ##So this is a lot different than thinking about data from excel\nLets try to do the summary stats on the data now and see how it differs\n\n# the data you want to look at\nskim(iris.df)\n\n\nData summary\n\n\nName\niris.df\n\n\nNumber of rows\n150\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nspecies\n0\n1\n6\n10\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsepal_length\n1\n0.99\n5.85\n0.83\n4.3\n5.1\n5.80\n6.4\n7.9\n▆▇▇▅▂\n\n\nsepal_width\n0\n1.00\n3.06\n0.44\n2.0\n2.8\n3.00\n3.3\n4.4\n▁▆▇▂▁\n\n\npetal_length\n0\n1.00\n3.76\n1.77\n1.0\n1.6\n4.35\n5.1\n6.9\n▇▁▆▇▂\n\n\npetal_width\n0\n1.00\n1.20\n0.76\n0.1\n0.3\n1.30\n1.8\n2.5\n▇▁▇▅▃\n\n\n\n\n\n\n\n\n\n# this will add an index to the dataframe so you know what individual is which\niris_long.df &lt;- iris.df %&gt;% \n  mutate(sample = row_number()) %&gt;%\n  gather(part, measure, -species, -sample)\n\n\n\n\n\n# Box Plots of data\niris_long.df %&gt;% \n  ggplot(aes(x = part, y = measure, color = species, fill=species))+\n  geom_boxplot(alpha=0.3) \n\n\n\n\n\n\n\n\n\n\n\nSo I think this is premature but some people like to test the normality of the data but really you should be assessing the normality of the residuals. But here it goes…\n\n# turn off scientific notaton\noptions(scipen = 999)\n# to turn back on \n#options(scipen = 0)\n\n# Test for normality of each group and store in shapirowilktests\n# This uses the broom package to get clean output of the test \niris_long.df %&gt;% group_by(species, part) %&gt;% do(tidy(shapiro.test(.$measure)))\n\n# A tibble: 12 × 5\n# Groups:   species, part [12]\n   species    part         statistic     p.value method                     \n   &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;                      \n 1 setosa     petal_length     0.955 0.0548      Shapiro-Wilk normality test\n 2 setosa     petal_width      0.800 0.000000866 Shapiro-Wilk normality test\n 3 setosa     sepal_length     0.977 0.456       Shapiro-Wilk normality test\n 4 setosa     sepal_width      0.972 0.272       Shapiro-Wilk normality test\n 5 versicolor petal_length     0.966 0.158       Shapiro-Wilk normality test\n 6 versicolor petal_width      0.948 0.0273      Shapiro-Wilk normality test\n 7 versicolor sepal_length     0.978 0.465       Shapiro-Wilk normality test\n 8 versicolor sepal_width      0.974 0.338       Shapiro-Wilk normality test\n 9 virginica  petal_length     0.962 0.110       Shapiro-Wilk normality test\n10 virginica  petal_width      0.960 0.0870      Shapiro-Wilk normality test\n11 virginica  sepal_length     0.971 0.258       Shapiro-Wilk normality test\n12 virginica  sepal_width      0.967 0.181       Shapiro-Wilk normality test\n\n\n\n\n\nThis info is from:\nhttp://stackoverflow.com/questions/29697009/correlation-matrix-plot-with-ggplot2\nand\nhttps://www.r-bloggers.com/plot-matrix-with-the-r-package-ggally/\nand\nhttp://ggobi.github.io/ggally/#canonical_correlation_analysis\n\niris.df %&gt;% \n  select(sepal_length, sepal_width, petal_length, petal_width) %&gt;%\n  ggpairs()\n\n\n\n\n\n\n\n\n##Look at the correlation matrix\n\n# correlation matrix of the data with only the numeric data in a dataframe\n# the old way - the same really\n# cor(setosa.df[,1:4], method = \"pearson\") # , method = c(\"pearson\", \"kendall\", \"spearman\")\n# need to only have numeric varaibles\niris.df %&gt;% select(-species) %&gt;% cor() \n\n             sepal_length sepal_width petal_length petal_width\nsepal_length            1          NA           NA          NA\nsepal_width            NA   1.0000000   -0.4284401  -0.3661259\npetal_length           NA  -0.4284401    1.0000000   0.9628654\npetal_width            NA  -0.3661259    0.9628654   1.0000000\n\n\n\n\n\n\npetals.cor &lt;- cor.test(iris.df$petal_length, iris.df$petal_width)\n\n# can see by calling model\npetals.cor\n\n\n    Pearson's product-moment correlation\n\ndata:  iris.df$petal_length and iris.df$petal_width\nt = 43.387, df = 148, p-value &lt; 0.00000000000000022\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9490525 0.9729853\nsample estimates:\n      cor \n0.9628654 \n\n\n\n# other way\ncor.test(~ petal_length + petal_width, iris.df)\n\n\n    Pearson's product-moment correlation\n\ndata:  petal_length and petal_width\nt = 43.387, df = 148, p-value &lt; 0.00000000000000022\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9490525 0.9729853\nsample estimates:\n      cor \n0.9628654 \n\n\n\n# You can extract values from the cor.test() object like this:\n\npetals.cor$estimate\n\n      cor \n0.9628654 \n\npetals.cor$p.value\n\n[1] 0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000004675004\n\n\n\n# This calculates the correlation coefficient and the degrees of freedom\niris.df %&gt;% summarize(petal_cor = cor.test(petal_length, petal_width)$estimate,\n                   nuts_df = cor.test(petal_length, petal_width)$parameter,\n                   nuts.pvalue = cor.test(petal_length, petal_width)$p.value)\n\n# A tibble: 1 × 3\n  petal_cor nuts_df nuts.pvalue\n      &lt;dbl&gt;   &lt;int&gt;       &lt;dbl&gt;\n1     0.963     148    4.68e-86\n\n\n\niris.df %&gt;%  do(tidy(cor.test(.$petal_length, .$petal_width))) \n\n# A tibble: 1 × 8\n  estimate statistic  p.value parameter conf.low conf.high method    alternative\n     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;      \n1    0.963      43.4 4.68e-86       148    0.949     0.973 Pearson'… two.sided  \n\n# can be done with grouping variables as well"
  },
  {
    "objectID": "15_correlations.html#correlation",
    "href": "15_correlations.html#correlation",
    "title": "Correlations",
    "section": "",
    "text": "##Load libraries We will read in the main files and load the libraries as we have worked with so far.\n\n# One new package for summary stats\n# install.packages(\"broom\")\n# install.packages(\"GGally\")\n# install.packages(\"car\")\n# install.packages(\"gvlma\")\n# install.packages(\"corrplot\")\n# install.packages(\"gvlma\")\n\n# load the libraries each time you restart R\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(skimr)\nlibrary(janitor)\nlibrary(patchwork)\n# library(reshape2)\nlibrary(broom)\nlibrary(GGally)\nlibrary(corrplot)\nlibrary(car)\nlibrary(gvlma)\n\n\n# read in the file\niris.df &lt;- read_csv(\"data/iris.csv\") %&gt;%\n  clean_names()\n\nglimpse(iris.df)\n\nRows: 150\nColumns: 5\n$ sepal_length &lt;dbl&gt; 5.1, NA, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.8…\n$ sepal_width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ petal_length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ petal_width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ species      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa…\n\n\n##Summary Statistics for the better look ##So this is a lot different than thinking about data from excel\nLets try to do the summary stats on the data now and see how it differs\n\n# the data you want to look at\nskim(iris.df)\n\n\nData summary\n\n\nName\niris.df\n\n\nNumber of rows\n150\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nspecies\n0\n1\n6\n10\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsepal_length\n1\n0.99\n5.85\n0.83\n4.3\n5.1\n5.80\n6.4\n7.9\n▆▇▇▅▂\n\n\nsepal_width\n0\n1.00\n3.06\n0.44\n2.0\n2.8\n3.00\n3.3\n4.4\n▁▆▇▂▁\n\n\npetal_length\n0\n1.00\n3.76\n1.77\n1.0\n1.6\n4.35\n5.1\n6.9\n▇▁▆▇▂\n\n\npetal_width\n0\n1.00\n1.20\n0.76\n0.1\n0.3\n1.30\n1.8\n2.5\n▇▁▇▅▃"
  },
  {
    "objectID": "15_correlations.html#long-to-wide-format",
    "href": "15_correlations.html#long-to-wide-format",
    "title": "Correlations",
    "section": "",
    "text": "# this will add an index to the dataframe so you know what individual is which\niris_long.df &lt;- iris.df %&gt;% \n  mutate(sample = row_number()) %&gt;%\n  gather(part, measure, -species, -sample)"
  },
  {
    "objectID": "15_correlations.html#outliers",
    "href": "15_correlations.html#outliers",
    "title": "Correlations",
    "section": "",
    "text": "# Box Plots of data\niris_long.df %&gt;% \n  ggplot(aes(x = part, y = measure, color = species, fill=species))+\n  geom_boxplot(alpha=0.3)"
  },
  {
    "objectID": "15_correlations.html#test-for-normality-of-data-and-using-the-broom-package",
    "href": "15_correlations.html#test-for-normality-of-data-and-using-the-broom-package",
    "title": "Correlations",
    "section": "",
    "text": "So I think this is premature but some people like to test the normality of the data but really you should be assessing the normality of the residuals. But here it goes…\n\n# turn off scientific notaton\noptions(scipen = 999)\n# to turn back on \n#options(scipen = 0)\n\n# Test for normality of each group and store in shapirowilktests\n# This uses the broom package to get clean output of the test \niris_long.df %&gt;% group_by(species, part) %&gt;% do(tidy(shapiro.test(.$measure)))\n\n# A tibble: 12 × 5\n# Groups:   species, part [12]\n   species    part         statistic     p.value method                     \n   &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;                      \n 1 setosa     petal_length     0.955 0.0548      Shapiro-Wilk normality test\n 2 setosa     petal_width      0.800 0.000000866 Shapiro-Wilk normality test\n 3 setosa     sepal_length     0.977 0.456       Shapiro-Wilk normality test\n 4 setosa     sepal_width      0.972 0.272       Shapiro-Wilk normality test\n 5 versicolor petal_length     0.966 0.158       Shapiro-Wilk normality test\n 6 versicolor petal_width      0.948 0.0273      Shapiro-Wilk normality test\n 7 versicolor sepal_length     0.978 0.465       Shapiro-Wilk normality test\n 8 versicolor sepal_width      0.974 0.338       Shapiro-Wilk normality test\n 9 virginica  petal_length     0.962 0.110       Shapiro-Wilk normality test\n10 virginica  petal_width      0.960 0.0870      Shapiro-Wilk normality test\n11 virginica  sepal_length     0.971 0.258       Shapiro-Wilk normality test\n12 virginica  sepal_width      0.967 0.181       Shapiro-Wilk normality test"
  },
  {
    "objectID": "15_correlations.html#correlations-plots",
    "href": "15_correlations.html#correlations-plots",
    "title": "Correlations",
    "section": "",
    "text": "This info is from:\nhttp://stackoverflow.com/questions/29697009/correlation-matrix-plot-with-ggplot2\nand\nhttps://www.r-bloggers.com/plot-matrix-with-the-r-package-ggally/\nand\nhttp://ggobi.github.io/ggally/#canonical_correlation_analysis\n\niris.df %&gt;% \n  select(sepal_length, sepal_width, petal_length, petal_width) %&gt;%\n  ggpairs()\n\n\n\n\n\n\n\n\n##Look at the correlation matrix\n\n# correlation matrix of the data with only the numeric data in a dataframe\n# the old way - the same really\n# cor(setosa.df[,1:4], method = \"pearson\") # , method = c(\"pearson\", \"kendall\", \"spearman\")\n# need to only have numeric varaibles\niris.df %&gt;% select(-species) %&gt;% cor() \n\n             sepal_length sepal_width petal_length petal_width\nsepal_length            1          NA           NA          NA\nsepal_width            NA   1.0000000   -0.4284401  -0.3661259\npetal_length           NA  -0.4284401    1.0000000   0.9628654\npetal_width            NA  -0.3661259    0.9628654   1.0000000"
  },
  {
    "objectID": "15_correlations.html#correlation-test",
    "href": "15_correlations.html#correlation-test",
    "title": "Correlations",
    "section": "",
    "text": "petals.cor &lt;- cor.test(iris.df$petal_length, iris.df$petal_width)\n\n# can see by calling model\npetals.cor\n\n\n    Pearson's product-moment correlation\n\ndata:  iris.df$petal_length and iris.df$petal_width\nt = 43.387, df = 148, p-value &lt; 0.00000000000000022\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9490525 0.9729853\nsample estimates:\n      cor \n0.9628654 \n\n\n\n# other way\ncor.test(~ petal_length + petal_width, iris.df)\n\n\n    Pearson's product-moment correlation\n\ndata:  petal_length and petal_width\nt = 43.387, df = 148, p-value &lt; 0.00000000000000022\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9490525 0.9729853\nsample estimates:\n      cor \n0.9628654 \n\n\n\n# You can extract values from the cor.test() object like this:\n\npetals.cor$estimate\n\n      cor \n0.9628654 \n\npetals.cor$p.value\n\n[1] 0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000004675004\n\n\n\n# This calculates the correlation coefficient and the degrees of freedom\niris.df %&gt;% summarize(petal_cor = cor.test(petal_length, petal_width)$estimate,\n                   nuts_df = cor.test(petal_length, petal_width)$parameter,\n                   nuts.pvalue = cor.test(petal_length, petal_width)$p.value)\n\n# A tibble: 1 × 3\n  petal_cor nuts_df nuts.pvalue\n      &lt;dbl&gt;   &lt;int&gt;       &lt;dbl&gt;\n1     0.963     148    4.68e-86\n\n\n\niris.df %&gt;%  do(tidy(cor.test(.$petal_length, .$petal_width))) \n\n# A tibble: 1 × 8\n  estimate statistic  p.value parameter conf.low conf.high method    alternative\n     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;      \n1    0.963      43.4 4.68e-86       148    0.949     0.973 Pearson'… two.sided  \n\n# can be done with grouping variables as well"
  },
  {
    "objectID": "14_t_test.html",
    "href": "14_t_test.html",
    "title": "T-Test",
    "section": "",
    "text": "The goal of this page is to learn to perform a t-test\n\n\n##Load libraries We will read in the main files and load the libraries as we have worked with so far.\n\n# One new package for summary stats\n#install.packages(\"broom\")\n# install.packages(\"GGally\")\n# install.packages(\"car\")\n# install.packages(\"gvlma\")\n\n\n# load the libraries each time you restart R\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(skimr)\nlibrary(janitor)\nlibrary(patchwork)\n# library(reshape2)\nlibrary(broom)\nlibrary(GGally)\nlibrary(corrplot)\nlibrary(car)\n\n\n\n\n\n# read in the file\niris.df &lt;- read_csv(\"data/iris.csv\") %&gt;%\n  clean_names() %&gt;%\n  remove_empty(c(\"rows\", \"cols\")) \n\nglimpse(iris.df)\n\nRows: 150\nColumns: 5\n$ sepal_length &lt;dbl&gt; 5.1, NA, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.8…\n$ sepal_width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ petal_length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ petal_width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ species      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa…\n\n\n###Summary Statistics for the better look ###So this is a lot different than thinking about data from excel\nLets try to do the summary stats on the data now and see how it differs\n\n# the data you want to look at\nskim(iris.df)\n\n\nData summary\n\n\nName\niris.df\n\n\nNumber of rows\n150\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nspecies\n0\n1\n6\n10\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsepal_length\n1\n0.99\n5.85\n0.83\n4.3\n5.1\n5.80\n6.4\n7.9\n▆▇▇▅▂\n\n\nsepal_width\n0\n1.00\n3.06\n0.44\n2.0\n2.8\n3.00\n3.3\n4.4\n▁▆▇▂▁\n\n\npetal_length\n0\n1.00\n3.76\n1.77\n1.0\n1.6\n4.35\n5.1\n6.9\n▇▁▆▇▂\n\n\npetal_width\n0\n1.00\n1.20\n0.76\n0.1\n0.3\n1.30\n1.8\n2.5\n▇▁▇▅▃\n\n\n\n\n\n\niris.df %&gt;% group_by(species) %&gt;% skim_to_wide()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n150\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nspecies\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nspecies\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsepal_length\nsetosa\n1\n0.98\n5.01\n0.36\n4.3\n4.80\n5.00\n5.20\n5.8\n▃▃▇▅▁\n\n\nsepal_length\nversicolor\n0\n1.00\n5.94\n0.52\n4.9\n5.60\n5.90\n6.30\n7.0\n▂▇▆▃▃\n\n\nsepal_length\nvirginica\n0\n1.00\n6.59\n0.64\n4.9\n6.23\n6.50\n6.90\n7.9\n▁▃▇▃▂\n\n\nsepal_width\nsetosa\n0\n1.00\n3.43\n0.38\n2.3\n3.20\n3.40\n3.68\n4.4\n▁▃▇▅▂\n\n\nsepal_width\nversicolor\n0\n1.00\n2.77\n0.31\n2.0\n2.52\n2.80\n3.00\n3.4\n▁▅▆▇▂\n\n\nsepal_width\nvirginica\n0\n1.00\n2.97\n0.32\n2.2\n2.80\n3.00\n3.18\n3.8\n▂▆▇▅▁\n\n\npetal_length\nsetosa\n0\n1.00\n1.46\n0.17\n1.0\n1.40\n1.50\n1.58\n1.9\n▁▃▇▃▁\n\n\npetal_length\nversicolor\n0\n1.00\n4.26\n0.47\n3.0\n4.00\n4.35\n4.60\n5.1\n▂▂▇▇▆\n\n\npetal_length\nvirginica\n0\n1.00\n5.55\n0.55\n4.5\n5.10\n5.55\n5.88\n6.9\n▃▇▇▃▂\n\n\npetal_width\nsetosa\n0\n1.00\n0.25\n0.11\n0.1\n0.20\n0.20\n0.30\n0.6\n▇▂▂▁▁\n\n\npetal_width\nversicolor\n0\n1.00\n1.33\n0.20\n1.0\n1.20\n1.30\n1.50\n1.8\n▅▇▃▆▁\n\n\npetal_width\nvirginica\n0\n1.00\n2.03\n0.27\n1.4\n1.80\n2.00\n2.30\n2.5\n▂▇▆▅▇\n\n\n\n\n\n\n\n\n\n# this will add an index to the dataframe so you know what individual is which\niris_long.df &lt;- iris.df %&gt;% \n  mutate(individual = row_number()) %&gt;% \n  gather(trait, measure, -species, - individual)\n\nhead(iris_long.df)\n\n# A tibble: 6 × 4\n  species individual trait        measure\n  &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;          &lt;dbl&gt;\n1 setosa           1 sepal_length     5.1\n2 setosa           2 sepal_length    NA  \n3 setosa           3 sepal_length     4.7\n4 setosa           4 sepal_length     4.6\n5 setosa           5 sepal_length     5  \n6 setosa           6 sepal_length     5.4\n\n\n\n\n\n\n# Box Plots of data\niris_long.df %&gt;% group_by(species, trait) %&gt;% \n  ggplot( aes(x = trait, y = measure, color = species, fill=species))+\n  geom_boxplot(aes(alpha=0.3)) \n\n\n\n\n\n\n\n\nThese look good for the most part with only one or two significant. Your choice to transform or not.\n##Factors\nMake sure the categorical variable is a factor\nRearrange the order of groups so that control group is first, followed by treatment groups.\nyou can do this by:\n\n# Make Factors from the different levels long way\n# iris_long.df$species &lt;- as.factor(iris_long.df$species)  \n# iris_long.df$trait &lt;- as.factor(iris_long.df$trait) \n\n# Make Factors dplyr\niris_long.df &lt;- iris_long.df %&gt;%\n  mutate(\n    sex = factor(species, \n    labels = c(\"setosa\", \"versicolor\", \"virginica\")),\n    trait = factor(trait, \n    labels = c(\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"))\n  )\n\n\n\n\nSo I think this is premature but some people like to test the normality of the data but really you should be assessing the normality of the residuals. But here it goes…\n\n# turn off scientific notaton\noptions(scipen = 999)\n# to turn back on \n#options(scipen = 0)\n\n# Test for normality of each group and store in shapirowilktests\n# This uses the broom package to get clean output of the test \niris_long.df %&gt;% group_by(species, trait) %&gt;% do(tidy(shapiro.test(.$measure)))\n\n# A tibble: 12 × 5\n# Groups:   species, trait [12]\n   species    trait        statistic     p.value method                     \n   &lt;chr&gt;      &lt;fct&gt;            &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;                      \n 1 setosa     sepal_length     0.955 0.0548      Shapiro-Wilk normality test\n 2 setosa     sepal_width      0.800 0.000000866 Shapiro-Wilk normality test\n 3 setosa     petal_length     0.977 0.456       Shapiro-Wilk normality test\n 4 setosa     petal_width      0.972 0.272       Shapiro-Wilk normality test\n 5 versicolor sepal_length     0.966 0.158       Shapiro-Wilk normality test\n 6 versicolor sepal_width      0.948 0.0273      Shapiro-Wilk normality test\n 7 versicolor petal_length     0.978 0.465       Shapiro-Wilk normality test\n 8 versicolor petal_width      0.974 0.338       Shapiro-Wilk normality test\n 9 virginica  sepal_length     0.962 0.110       Shapiro-Wilk normality test\n10 virginica  sepal_width      0.960 0.0870      Shapiro-Wilk normality test\n11 virginica  petal_length     0.971 0.258       Shapiro-Wilk normality test\n12 virginica  petal_width      0.967 0.181       Shapiro-Wilk normality test\n\n#You can do this on all variables faster with if there was only one grouping\n# tapply(iris_long.df$measure, iris_long.df$species, shapiro.test)\n\n##Test for homogenetiy of variances\nBest to use the Levenes test compared to the Bartlet test\nuses the car package\nhttp://www.cookbook-r.com/Statistical_analysis/Homogeneity_of_variance/\nNote however that this is doing the homogeneity test on all traits and not each one which is what you really should do and is not often done. Need to check on this though.\n\n#Test for homogeneity of variances by groups\nleveneTest(sepal_length ~ species, data=iris.df)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value   Pr(&gt;F)   \ngroup   2  6.0386 0.003022 **\n      146                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#Compared to the Bartlet test\nbartlett.test(sepal_length ~ species, data=iris.df)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  sepal_length by species\nBartlett's K-squared = 15.308, df = 2, p-value = 0.0004742\n\n\nCreate a subset dataframe\n\nsepal_length.df &lt;- iris_long.df %&gt;%\n  filter(trait ==\"sepal_length\") %&gt;%\n  filter(species != \"setosa\")\n\nIf p-value &gt;= 0.05, use var.equal=TRUE below\n\nt.test(measure ~ species, data=sepal_length.df,\n       var.equal=TRUE,\n       conf.level=0.95)\n\n\n    Two Sample t-test\n\ndata:  measure by species\nt = -12.604, df = 98, p-value &lt; 0.00000000000000022\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -1.495426 -1.088574\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   4.260                    5.552 \n\n\n\nt.test(measure ~ species, data=sepal_length.df,\n       var.equal=FALSE,\n       conf.level=0.95)\n\n\n    Welch Two Sample t-test\n\ndata:  measure by species\nt = -12.604, df = 95.57, p-value &lt; 0.00000000000000022\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -1.49549 -1.08851\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   4.260                    5.552 \n\n\nBoxplot of data\n\nsepal_length.df %&gt;%\n  ggplot(aes(species, measure, fill = species)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nMean and standard error plot\n\nsepal_length.df %&gt;% \nggplot(aes(species, color=species)) + \n  stat_summary(aes(y = measure),\n               fun.y = mean, na.rm = TRUE,\n               geom = \"point\",\n               size = 3) + \n  stat_summary(aes(y = measure),\n               fun.data = mean_se, na.rm = TRUE,\n               geom = \"errorbar\",\n               width = 0.2) +\n  labs(x = \"Species\", y = \"Length\") +\n  scale_color_manual(name = \"Species\", \n                     values = c(\"blue\", \"red\"),\n                     labels = c(\"Versicolor\", \"Virginica\"))"
  },
  {
    "objectID": "14_t_test.html#t-tests",
    "href": "14_t_test.html#t-tests",
    "title": "T-Test",
    "section": "",
    "text": "##Load libraries We will read in the main files and load the libraries as we have worked with so far.\n\n# One new package for summary stats\n#install.packages(\"broom\")\n# install.packages(\"GGally\")\n# install.packages(\"car\")\n# install.packages(\"gvlma\")\n\n\n# load the libraries each time you restart R\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(skimr)\nlibrary(janitor)\nlibrary(patchwork)\n# library(reshape2)\nlibrary(broom)\nlibrary(GGally)\nlibrary(corrplot)\nlibrary(car)"
  },
  {
    "objectID": "14_t_test.html#read-in-file",
    "href": "14_t_test.html#read-in-file",
    "title": "T-Test",
    "section": "",
    "text": "# read in the file\niris.df &lt;- read_csv(\"data/iris.csv\") %&gt;%\n  clean_names() %&gt;%\n  remove_empty(c(\"rows\", \"cols\")) \n\nglimpse(iris.df)\n\nRows: 150\nColumns: 5\n$ sepal_length &lt;dbl&gt; 5.1, NA, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.8…\n$ sepal_width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ petal_length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ petal_width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ species      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa…\n\n\n###Summary Statistics for the better look ###So this is a lot different than thinking about data from excel\nLets try to do the summary stats on the data now and see how it differs\n\n# the data you want to look at\nskim(iris.df)\n\n\nData summary\n\n\nName\niris.df\n\n\nNumber of rows\n150\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nspecies\n0\n1\n6\n10\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsepal_length\n1\n0.99\n5.85\n0.83\n4.3\n5.1\n5.80\n6.4\n7.9\n▆▇▇▅▂\n\n\nsepal_width\n0\n1.00\n3.06\n0.44\n2.0\n2.8\n3.00\n3.3\n4.4\n▁▆▇▂▁\n\n\npetal_length\n0\n1.00\n3.76\n1.77\n1.0\n1.6\n4.35\n5.1\n6.9\n▇▁▆▇▂\n\n\npetal_width\n0\n1.00\n1.20\n0.76\n0.1\n0.3\n1.30\n1.8\n2.5\n▇▁▇▅▃\n\n\n\n\n\n\niris.df %&gt;% group_by(species) %&gt;% skim_to_wide()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n150\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nspecies\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nspecies\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsepal_length\nsetosa\n1\n0.98\n5.01\n0.36\n4.3\n4.80\n5.00\n5.20\n5.8\n▃▃▇▅▁\n\n\nsepal_length\nversicolor\n0\n1.00\n5.94\n0.52\n4.9\n5.60\n5.90\n6.30\n7.0\n▂▇▆▃▃\n\n\nsepal_length\nvirginica\n0\n1.00\n6.59\n0.64\n4.9\n6.23\n6.50\n6.90\n7.9\n▁▃▇▃▂\n\n\nsepal_width\nsetosa\n0\n1.00\n3.43\n0.38\n2.3\n3.20\n3.40\n3.68\n4.4\n▁▃▇▅▂\n\n\nsepal_width\nversicolor\n0\n1.00\n2.77\n0.31\n2.0\n2.52\n2.80\n3.00\n3.4\n▁▅▆▇▂\n\n\nsepal_width\nvirginica\n0\n1.00\n2.97\n0.32\n2.2\n2.80\n3.00\n3.18\n3.8\n▂▆▇▅▁\n\n\npetal_length\nsetosa\n0\n1.00\n1.46\n0.17\n1.0\n1.40\n1.50\n1.58\n1.9\n▁▃▇▃▁\n\n\npetal_length\nversicolor\n0\n1.00\n4.26\n0.47\n3.0\n4.00\n4.35\n4.60\n5.1\n▂▂▇▇▆\n\n\npetal_length\nvirginica\n0\n1.00\n5.55\n0.55\n4.5\n5.10\n5.55\n5.88\n6.9\n▃▇▇▃▂\n\n\npetal_width\nsetosa\n0\n1.00\n0.25\n0.11\n0.1\n0.20\n0.20\n0.30\n0.6\n▇▂▂▁▁\n\n\npetal_width\nversicolor\n0\n1.00\n1.33\n0.20\n1.0\n1.20\n1.30\n1.50\n1.8\n▅▇▃▆▁\n\n\npetal_width\nvirginica\n0\n1.00\n2.03\n0.27\n1.4\n1.80\n2.00\n2.30\n2.5\n▂▇▆▅▇"
  },
  {
    "objectID": "14_t_test.html#wide-to-long-format",
    "href": "14_t_test.html#wide-to-long-format",
    "title": "T-Test",
    "section": "",
    "text": "# this will add an index to the dataframe so you know what individual is which\niris_long.df &lt;- iris.df %&gt;% \n  mutate(individual = row_number()) %&gt;% \n  gather(trait, measure, -species, - individual)\n\nhead(iris_long.df)\n\n# A tibble: 6 × 4\n  species individual trait        measure\n  &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;          &lt;dbl&gt;\n1 setosa           1 sepal_length     5.1\n2 setosa           2 sepal_length    NA  \n3 setosa           3 sepal_length     4.7\n4 setosa           4 sepal_length     4.6\n5 setosa           5 sepal_length     5  \n6 setosa           6 sepal_length     5.4"
  },
  {
    "objectID": "14_t_test.html#outliers",
    "href": "14_t_test.html#outliers",
    "title": "T-Test",
    "section": "",
    "text": "# Box Plots of data\niris_long.df %&gt;% group_by(species, trait) %&gt;% \n  ggplot( aes(x = trait, y = measure, color = species, fill=species))+\n  geom_boxplot(aes(alpha=0.3)) \n\n\n\n\n\n\n\n\nThese look good for the most part with only one or two significant. Your choice to transform or not.\n##Factors\nMake sure the categorical variable is a factor\nRearrange the order of groups so that control group is first, followed by treatment groups.\nyou can do this by:\n\n# Make Factors from the different levels long way\n# iris_long.df$species &lt;- as.factor(iris_long.df$species)  \n# iris_long.df$trait &lt;- as.factor(iris_long.df$trait) \n\n# Make Factors dplyr\niris_long.df &lt;- iris_long.df %&gt;%\n  mutate(\n    sex = factor(species, \n    labels = c(\"setosa\", \"versicolor\", \"virginica\")),\n    trait = factor(trait, \n    labels = c(\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"))\n  )"
  },
  {
    "objectID": "14_t_test.html#test-for-normality-of-data-and-using-the-broom-package",
    "href": "14_t_test.html#test-for-normality-of-data-and-using-the-broom-package",
    "title": "T-Test",
    "section": "",
    "text": "So I think this is premature but some people like to test the normality of the data but really you should be assessing the normality of the residuals. But here it goes…\n\n# turn off scientific notaton\noptions(scipen = 999)\n# to turn back on \n#options(scipen = 0)\n\n# Test for normality of each group and store in shapirowilktests\n# This uses the broom package to get clean output of the test \niris_long.df %&gt;% group_by(species, trait) %&gt;% do(tidy(shapiro.test(.$measure)))\n\n# A tibble: 12 × 5\n# Groups:   species, trait [12]\n   species    trait        statistic     p.value method                     \n   &lt;chr&gt;      &lt;fct&gt;            &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;                      \n 1 setosa     sepal_length     0.955 0.0548      Shapiro-Wilk normality test\n 2 setosa     sepal_width      0.800 0.000000866 Shapiro-Wilk normality test\n 3 setosa     petal_length     0.977 0.456       Shapiro-Wilk normality test\n 4 setosa     petal_width      0.972 0.272       Shapiro-Wilk normality test\n 5 versicolor sepal_length     0.966 0.158       Shapiro-Wilk normality test\n 6 versicolor sepal_width      0.948 0.0273      Shapiro-Wilk normality test\n 7 versicolor petal_length     0.978 0.465       Shapiro-Wilk normality test\n 8 versicolor petal_width      0.974 0.338       Shapiro-Wilk normality test\n 9 virginica  sepal_length     0.962 0.110       Shapiro-Wilk normality test\n10 virginica  sepal_width      0.960 0.0870      Shapiro-Wilk normality test\n11 virginica  petal_length     0.971 0.258       Shapiro-Wilk normality test\n12 virginica  petal_width      0.967 0.181       Shapiro-Wilk normality test\n\n#You can do this on all variables faster with if there was only one grouping\n# tapply(iris_long.df$measure, iris_long.df$species, shapiro.test)\n\n##Test for homogenetiy of variances\nBest to use the Levenes test compared to the Bartlet test\nuses the car package\nhttp://www.cookbook-r.com/Statistical_analysis/Homogeneity_of_variance/\nNote however that this is doing the homogeneity test on all traits and not each one which is what you really should do and is not often done. Need to check on this though.\n\n#Test for homogeneity of variances by groups\nleveneTest(sepal_length ~ species, data=iris.df)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value   Pr(&gt;F)   \ngroup   2  6.0386 0.003022 **\n      146                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#Compared to the Bartlet test\nbartlett.test(sepal_length ~ species, data=iris.df)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  sepal_length by species\nBartlett's K-squared = 15.308, df = 2, p-value = 0.0004742\n\n\nCreate a subset dataframe\n\nsepal_length.df &lt;- iris_long.df %&gt;%\n  filter(trait ==\"sepal_length\") %&gt;%\n  filter(species != \"setosa\")\n\nIf p-value &gt;= 0.05, use var.equal=TRUE below\n\nt.test(measure ~ species, data=sepal_length.df,\n       var.equal=TRUE,\n       conf.level=0.95)\n\n\n    Two Sample t-test\n\ndata:  measure by species\nt = -12.604, df = 98, p-value &lt; 0.00000000000000022\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -1.495426 -1.088574\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   4.260                    5.552 \n\n\n\nt.test(measure ~ species, data=sepal_length.df,\n       var.equal=FALSE,\n       conf.level=0.95)\n\n\n    Welch Two Sample t-test\n\ndata:  measure by species\nt = -12.604, df = 95.57, p-value &lt; 0.00000000000000022\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -1.49549 -1.08851\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   4.260                    5.552 \n\n\nBoxplot of data\n\nsepal_length.df %&gt;%\n  ggplot(aes(species, measure, fill = species)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nMean and standard error plot\n\nsepal_length.df %&gt;% \nggplot(aes(species, color=species)) + \n  stat_summary(aes(y = measure),\n               fun.y = mean, na.rm = TRUE,\n               geom = \"point\",\n               size = 3) + \n  stat_summary(aes(y = measure),\n               fun.data = mean_se, na.rm = TRUE,\n               geom = \"errorbar\",\n               width = 0.2) +\n  labs(x = \"Species\", y = \"Length\") +\n  scale_color_manual(name = \"Species\", \n                     values = c(\"blue\", \"red\"),\n                     labels = c(\"Versicolor\", \"Virginica\"))"
  },
  {
    "objectID": "index.html#topics-covered",
    "href": "index.html#topics-covered",
    "title": "Intro Stats 2024",
    "section": "Topics Covered",
    "text": "Topics Covered\n\nR_esources that can help you a lot\nlink to resources\n\n\nSetting up R and RStudio\ninstalling R\n\n\nInstalling packages/libraries\nhow to do this\n\n\nProject set up if using raw code\nprojects\n\n\nReading and writing files into R\nReading and Writing\n\n\nBasics of GGplot\nThe basics\n\n\nCustomizing colors and shapes in GGPlot\nhow to change things\n\n\nMaking Mean and SE plots in GGPLOT\nEasy Mean SE Plots\n\n\nMultiple panel plots\nUsing patchwork for multiple plots\n\n\nSelect and filter to change datframes\nHow to select columns and filter rows\n\n\nSummarizing data\nHow to summarize mean and SE values\n\n\nManipulating factors\nHow to work with Factors and Forecats and reorder\n\n\nJoining dataframes\nHow to bring two datafames together\n\n\nT-Tests\nHow to do a T - Test\n\n\nCorrelations\nHow to do a correlation\n\n\nRegression\nHow to do a regression\n\n\nOne Way ANOVA\nHow to do a One Way ANOVA\n\n\nTwo Way ANOVA\nHow to do a Two Way ANOVA"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am an aquatic ecologist working on both chemistry and biology of frewshwaters.\n\n\nIf you are interested in looking at my publications - they can be found on Research Gate (Here) or on google scholar (Here)\n\n\n\n\n\n\nContact Infoformation Bill Perry Department of Biology Illinois State University Campus Box 4120\n\n\n309-438-8160 wlperry and the rest is ilstu dot edu"
  },
  {
    "objectID": "16_regression.html",
    "href": "16_regression.html",
    "title": "Regression",
    "section": "",
    "text": "The goal of this page is to learn to perform a regression\n\n\n##An excellent description of the stats for AOV and regression is here:\nhttps://www.zoology.ubc.ca/~schluter/R/fit-model/\n##Load libraries We will read in the main files and load the libraries as we have worked with so far.\n\n# One new package for summary stats\n# install.packages(\"broom\")\n# install.packages(\"GGally\")\n# install.packages(\"car\")\n# install.packages(\"gvlma\")\n# install.packages(\"corrplot\")\n# install.packages(\"gvlma\")\n\n# load the libraries each time you restart R\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(skimr)\nlibrary(janitor)\nlibrary(patchwork)\n# library(reshape2)\nlibrary(broom)\nlibrary(GGally)\nlibrary(corrplot)\nlibrary(car)\nlibrary(gvlma)"
  },
  {
    "objectID": "16_regression.html#regression",
    "href": "16_regression.html#regression",
    "title": "Regression",
    "section": "",
    "text": "##An excellent description of the stats for AOV and regression is here:\nhttps://www.zoology.ubc.ca/~schluter/R/fit-model/\n##Load libraries We will read in the main files and load the libraries as we have worked with so far.\n\n# One new package for summary stats\n# install.packages(\"broom\")\n# install.packages(\"GGally\")\n# install.packages(\"car\")\n# install.packages(\"gvlma\")\n# install.packages(\"corrplot\")\n# install.packages(\"gvlma\")\n\n# load the libraries each time you restart R\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(skimr)\nlibrary(janitor)\nlibrary(patchwork)\n# library(reshape2)\nlibrary(broom)\nlibrary(GGally)\nlibrary(corrplot)\nlibrary(car)\nlibrary(gvlma)"
  },
  {
    "objectID": "16_regression.html#read-in-files",
    "href": "16_regression.html#read-in-files",
    "title": "Regression",
    "section": "Read in files",
    "text": "Read in files\n\nstds.df &lt;- read_csv(\"data/standards.csv\")\n\nglimpse(stds.df)\n\nRows: 30\nColumns: 5\n$ replicate &lt;dbl&gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, …\n$ std       &lt;dbl&gt; 0.000, 0.000, 0.005, 0.005, 0.010, 0.010, 0.020, 0.020, 0.03…\n$ drp       &lt;dbl&gt; 0.000, 0.000, 0.002, 0.006, 0.004, 0.003, 0.006, 0.006, NA, …\n$ tp        &lt;dbl&gt; -0.002, -0.002, -0.001, -0.001, 0.000, 0.000, 0.002, 0.003, …\n$ nh4       &lt;dbl&gt; 0.008, 0.008, NA, NA, 0.018, 0.018, 0.020, 0.020, 0.026, 0.0…"
  },
  {
    "objectID": "16_regression.html#standards-long-format",
    "href": "16_regression.html#standards-long-format",
    "title": "Regression",
    "section": "Standards long format",
    "text": "Standards long format\n\nsts_long.df &lt;- stds.df %&gt;%\n  gather(analyte, abs, -replicate, - std)"
  },
  {
    "objectID": "16_regression.html#linear-regression-ggplot",
    "href": "16_regression.html#linear-regression-ggplot",
    "title": "Regression",
    "section": "Linear Regression GGPlot",
    "text": "Linear Regression GGPlot\n\nstds.df %&gt;% \n  ggplot(aes(x=std, y=drp)) +\n  geom_point(size=2) +\n  geom_smooth(method=\"lm\")"
  },
  {
    "objectID": "16_regression.html#linear-regression",
    "href": "16_regression.html#linear-regression",
    "title": "Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear regression models\n\n# Fit our regression model\n# regression formula and dataframte\n\ndrp.model &lt;- lm(drp ~ std, data=stds.df) \n\n# Summarize and print the results\nsummary(drp.model) # show regression coefficients table\n\n\nCall:\nlm(formula = drp ~ std, data = stds.df)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0041414 -0.0008169  0.0000548  0.0001750  0.0036839 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.0008144  0.0004281   1.902   0.0716 .  \nstd         0.3003270  0.0009545 314.640   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.001534 on 20 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.9998,    Adjusted R-squared:  0.9998 \nF-statistic: 9.9e+04 on 1 and 20 DF,  p-value: &lt; 2.2e-16\n\n\nFrom this we would look at the values for slope = 0.3003270\nintercept = 0.0008144\nR^2 = 0.9998\n###AOV table of regression\n\nanova(drp.model)\n\nAnalysis of Variance Table\n\nResponse: drp\n          Df   Sum Sq  Mean Sq F value    Pr(&gt;F)    \nstd        1 0.232855 0.232855   98998 &lt; 2.2e-16 ***\nResiduals 20 0.000047 0.000002                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "16_regression.html#confidence-intervals-of-estimates",
    "href": "16_regression.html#confidence-intervals-of-estimates",
    "title": "Regression",
    "section": "Confidence intervals of estimates",
    "text": "Confidence intervals of estimates\n\n# Confidence intervals for the sepal model\nconfint(drp.model)\n\n                    2.5 %      97.5 %\n(Intercept) -0.0000786454 0.001707489\nstd          0.2983358854 0.302318032"
  },
  {
    "objectID": "16_regression.html#linear-regresson-assumptions",
    "href": "16_regression.html#linear-regresson-assumptions",
    "title": "Regression",
    "section": "Linear Regresson Assumptions",
    "text": "Linear Regresson Assumptions\nOrdinary least squares regression relies on several assumptions\n1. residuals are normally distributed and homoscedastic\n2. errors are independent\n3. relationships are linear\nInvestigate these assumptions visually by plotting your model:"
  },
  {
    "objectID": "16_regression.html#histogram-of-residuals",
    "href": "16_regression.html#histogram-of-residuals",
    "title": "Regression",
    "section": "Histogram of residuals",
    "text": "Histogram of residuals\n\n# histogram of residuals\nhist(residuals(drp.model))"
  },
  {
    "objectID": "16_regression.html#diagnostic-plots",
    "href": "16_regression.html#diagnostic-plots",
    "title": "Regression",
    "section": "Diagnostic Plots",
    "text": "Diagnostic Plots\n\npar(mar = c(4, 4, 2, 2), mfrow = c(1, 2)) \nplot(drp.model, which = c(1, 2)) # \"which\" argument optional"
  },
  {
    "objectID": "16_regression.html#simple_regplot",
    "href": "16_regression.html#simple_regplot",
    "title": "Regression",
    "section": "Plot of the Regression",
    "text": "Plot of the Regression\n\nplot(data=stds.df, drp ~ std, main=\"Regression Plot\")\nabline(drp.model, col=\"red\")"
  },
  {
    "objectID": "16_regression.html#nonconstant-error-variance-or-homoscedasticity",
    "href": "16_regression.html#nonconstant-error-variance-or-homoscedasticity",
    "title": "Regression",
    "section": "Non‐constant Error Variance or Homoscedasticity",
    "text": "Non‐constant Error Variance or Homoscedasticity\n\n# Evaluate homoscedasticity\n# non-constant error variance test\nncvTest(drp.model)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 5.294216, Df = 1, p = 0.021396"
  },
  {
    "objectID": "16_regression.html#test-for-normality-of-residuals",
    "href": "16_regression.html#test-for-normality-of-residuals",
    "title": "Regression",
    "section": "Test for normality of residuals",
    "text": "Test for normality of residuals\nto confirm the qqplot\n\n#Test for normality of residuals\nshapiro.test(drp.model$res)\n\n\n    Shapiro-Wilk normality test\n\ndata:  drp.model$res\nW = 0.84232, p-value = 0.00249"
  },
  {
    "objectID": "16_regression.html#different-code-for-a-qqplot-for-normality",
    "href": "16_regression.html#different-code-for-a-qqplot-for-normality",
    "title": "Regression",
    "section": "Different code for a QQPlot for normality",
    "text": "Different code for a QQPlot for normality\n\nqqPlot(drp.model, main=\"QQ Plot\") #qq plot for studentized resid\n\n\n\n\n\n\n\n\n[1]  4 30\n\n\n\nSave residuals for further analyses\n\n# # now to put the residuals next to the data and make sure that NAs are included\n# Not sure why it has an error but it works.\n# I am working on a nicer way to do this\n\nstds.df$residuals[!is.na(stds.df$std)]&lt;-residuals(lm(data=stds.df, drp ~ std, na.action=na.omit))\n\nhead(stds.df)\n\n# A tibble: 6 × 6\n  replicate   std   drp     tp    nh4 residuals\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1         1 0     0     -0.002  0.008 -0.000814\n2         2 0     0     -0.002  0.008 -0.000814\n3         1 0.005 0.002 -0.001 NA     -0.000316\n4         2 0.005 0.006 -0.001 NA      0.00368 \n5         1 0.01  0.004  0      0.018  0.000182\n6         2 0.01  0.003  0      0.018 -0.000818"
  },
  {
    "objectID": "16_regression.html#now-to-add-in-the-predicted-values",
    "href": "16_regression.html#now-to-add-in-the-predicted-values",
    "title": "Regression",
    "section": "Now to add in the predicted values",
    "text": "Now to add in the predicted values"
  },
  {
    "objectID": "16_regression.html#store-fitted-values-in-dataframe",
    "href": "16_regression.html#store-fitted-values-in-dataframe",
    "title": "Regression",
    "section": "Store fitted values in dataframe",
    "text": "Store fitted values in dataframe\nSo this is not working and I need to look into this more but this is in theory the way to do it.\n\n# now to see a plot of fitted and observed-----\nstds.df$fitted[!is.na(stds.df$std)] &lt;- fitted(lm(data=stds.df, drp ~ std, na.action=na.omit))\n\nhead(stds.df)\n\n# A tibble: 6 × 7\n  replicate   std   drp     tp    nh4 residuals   fitted\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1         1 0     0     -0.002  0.008 -0.000814 0.000814\n2         2 0     0     -0.002  0.008 -0.000814 0.000814\n3         1 0.005 0.002 -0.001 NA     -0.000316 0.00232 \n4         2 0.005 0.006 -0.001 NA      0.00368  0.00232 \n5         1 0.01  0.004  0      0.018  0.000182 0.00382 \n6         2 0.01  0.003  0      0.018 -0.000818 0.00382"
  },
  {
    "objectID": "16_regression.html#ggplot-of-data-and-fitted-values",
    "href": "16_regression.html#ggplot-of-data-and-fitted-values",
    "title": "Regression",
    "section": "GGPlot of data and fitted values",
    "text": "GGPlot of data and fitted values\n\nggplot(stds.df)  +\n    geom_point(aes(x = std, y = drp), color=\"blue\")+\n     geom_point(aes(x = std, y = fitted), color=\"red\")+\n    geom_line(aes(x = std, y = fitted), color=\"red\")\n\n\n\n\n\n\n\n\n##Other packages that do similiar things maybe better.\n###The gvlma package can do a lot of this automatically {#gvlma}\n\n #install.packages(\"gvlma\")\n# library(gvlma)\n\n# Global test of model assumptions\ngvmodel &lt;- gvlma(drp.model)\nsummary(gvmodel)\n\n\nCall:\nlm(formula = drp ~ std, data = stds.df)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0041414 -0.0008169  0.0000548  0.0001750  0.0036839 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.0008144  0.0004281   1.902   0.0716 .  \nstd         0.3003270  0.0009545 314.640   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.001534 on 20 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.9998,    Adjusted R-squared:  0.9998 \nF-statistic: 9.9e+04 on 1 and 20 DF,  p-value: &lt; 2.2e-16\n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = drp.model) \n\n                      Value p-value                   Decision\nGlobal Stat        11.05709 0.02593 Assumptions NOT satisfied!\nSkewness            0.01239 0.91138    Assumptions acceptable.\nKurtosis            6.20162 0.01276 Assumptions NOT satisfied!\nLink Function       3.00386 0.08307    Assumptions acceptable.\nHeteroscedasticity  1.83923 0.17504    Assumptions acceptable."
  },
  {
    "objectID": "01_resources.html",
    "href": "01_resources.html",
    "title": "R Resources",
    "section": "",
    "text": "Welcome to R Resources – a curated list of materials that I have found incredibly helpful in learning R. In this site, you’ll find links to books, websites, and forums that can guide you on your R journey.\n\n\n\nI highly recommend R for Data Science (2e) as a starting point.\n\n\n\n\n\nWhen you run into issues or have questions, StackOverflow is an excellent resource for quick answers and community support.\n\n\n\n\n\nFor a deeper dive into statistics with R, check out R Companion. It’s a fantastic guide to understanding and applying statistical methods in R."
  },
  {
    "objectID": "01_resources.html#featured-resource-r-for-data-science",
    "href": "01_resources.html#featured-resource-r-for-data-science",
    "title": "R Resources",
    "section": "",
    "text": "I highly recommend R for Data Science (2e) as a starting point."
  },
  {
    "objectID": "01_resources.html#getting-help-stackoverflow",
    "href": "01_resources.html#getting-help-stackoverflow",
    "title": "R Resources",
    "section": "",
    "text": "When you run into issues or have questions, StackOverflow is an excellent resource for quick answers and community support."
  },
  {
    "objectID": "01_resources.html#learning-statistics-with-r-companion",
    "href": "01_resources.html#learning-statistics-with-r-companion",
    "title": "R Resources",
    "section": "",
    "text": "For a deeper dive into statistics with R, check out R Companion. It’s a fantastic guide to understanding and applying statistical methods in R."
  },
  {
    "objectID": "02_installing_r.html",
    "href": "02_installing_r.html",
    "title": "Installing R",
    "section": "",
    "text": "Installing R is the first step to getting started with data analysis, while RStudio provides a powerful and user-friendly interface to work with R.\n\n\n\nR is a standalone program that does all the heavy lifting behind the scenes. To install R, follow these steps:\n\nDownload R from CRAN:\nVisit the CRAN website to download the version of R that matches your operating system. (Fun fact: R versions are named after Charlie Brown episodes!)\nTip: Check for updates every six months to ensure you have the latest features and bug fixes.\nRun the Installer:\nAfter downloading, run the installer and follow the on‑screen instructions.\n\n\n\n\n\n\n\n\n\n\nOnce you have R installed, it’s time to install RStudio—the Integrated Development Environment (IDE) that makes working with R much easier.\n\nDownload RStudio:\nVisit the RStudio download page to get the installer for your computer.\nInstall and Launch RStudio:\nAfter installing, open RStudio. It will automatically connect to the installed version of R, providing a streamlined interface to work with scripts, data, and visualizations.\n\n\n\nPositron a new and cool interface\nVisit the Positron page to get the installer for your computer."
  },
  {
    "objectID": "02_installing_r.html#installing-r",
    "href": "02_installing_r.html#installing-r",
    "title": "Installing R",
    "section": "",
    "text": "R is a standalone program that does all the heavy lifting behind the scenes. To install R, follow these steps:\n\nDownload R from CRAN:\nVisit the CRAN website to download the version of R that matches your operating system. (Fun fact: R versions are named after Charlie Brown episodes!)\nTip: Check for updates every six months to ensure you have the latest features and bug fixes.\nRun the Installer:\nAfter downloading, run the installer and follow the on‑screen instructions."
  },
  {
    "objectID": "02_installing_r.html#installing-rstudio",
    "href": "02_installing_r.html#installing-rstudio",
    "title": "Installing R",
    "section": "",
    "text": "Once you have R installed, it’s time to install RStudio—the Integrated Development Environment (IDE) that makes working with R much easier.\n\nDownload RStudio:\nVisit the RStudio download page to get the installer for your computer.\nInstall and Launch RStudio:\nAfter installing, open RStudio. It will automatically connect to the installed version of R, providing a streamlined interface to work with scripts, data, and visualizations.\n\n\n\nPositron a new and cool interface\nVisit the Positron page to get the installer for your computer."
  },
  {
    "objectID": "99_example_data_files.html",
    "href": "99_example_data_files.html",
    "title": "Example Data Files",
    "section": "",
    "text": "Below is a list of example data files available in the data directory:\n\n\n\nDataframe\nCSV version\nXLSX version\n\n\n\n\nBeetle Data\nbeetle.csv\nbeetle.xlsx\n\n\nGene Data\ngene_data.csv\n—\n\n\nIris Data\niris.csv\niris_excel.xlsx\n\n\nMendota Temperature\nmendota_temp.csv\n—\n\n\nM&M Data\nmms.csv\nmms.xlsx\n\n\nStandards\nstandards.csv\n—\n\n\nToolik Profile\ntoolik_profile.csv\n—\n\n\nVermillion Danville\nvermillion_danville.csv\n—"
  },
  {
    "objectID": "17_one_way_anova.html",
    "href": "17_one_way_anova.html",
    "title": "One Way ANOVA",
    "section": "",
    "text": "The goal of this page is to learn to perform an ANOVA\n##Load libraries We will read in the main files and load the libraries as we have worked with so far.\n\n# #Install Packages ----\n# install.packages(\"tidyverse\")\n# install.packages(\"lubridate\")\n# install.packages(\"scales\")\n# install.packages(\"readxl\")\n# install.packages(\"survminer\")\n# install.packages(\"survival\")\n# install.packages(\"patchwork\")\n# install.packages(\"broom\")\n# ANOVA specific\n# install.packages(\"car\")\n# install.packages(\"emmeans\")\n# install.packages(\"multcompView\")\n\n\n#Load libraries ----\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(readxl)\nlibrary(skimr)\nlibrary(broom)\nlibrary(janitor)\n# library(zoo)\nlibrary(patchwork)\nlibrary(car)\nlibrary(emmeans)\nlibrary(multcompView)\n\n\n\n\n# read in the file\ngene_exp.df &lt;- read_csv(\"data/gene_data.csv\") %&gt;%\n  clean_names() \n\nRows: 9 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): individual, replicate, control, treat1, treat2, treat3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(gene_exp.df)\n\nRows: 9\nColumns: 6\n$ individual &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9\n$ replicate  &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3\n$ control    &lt;dbl&gt; 12.59708, 12.48503, 12.04311, 12.46447, 12.02386, 12.65620,…\n$ treat1     &lt;dbl&gt; 20.87269, 19.57602, 15.66664, 15.70462, 20.20846, 18.15278,…\n$ treat2     &lt;dbl&gt; 14.94204, 15.90944, 13.39281, 17.80308, 20.25343, 19.59284,…\n$ treat3     &lt;dbl&gt; 11.90506, 12.42411, 12.34661, 11.93034, 11.78713, 12.07246,…\n\n\n##Summary Statistics for the better look\n\n# the data you want to look at\nskim(gene_exp.df)\n\n\nData summary\n\n\nName\ngene_exp.df\n\n\nNumber of rows\n9\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nindividual\n0\n1\n5.00\n2.74\n1.00\n3.00\n5.00\n7.00\n9.00\n▇▇▃▇▇\n\n\nreplicate\n0\n1\n2.00\n0.87\n1.00\n1.00\n2.00\n3.00\n3.00\n▇▁▇▁▇\n\n\ncontrol\n0\n1\n12.37\n0.24\n12.02\n12.16\n12.46\n12.53\n12.66\n▅▂▂▅▇\n\n\ntreat1\n0\n1\n17.31\n2.90\n12.27\n15.67\n18.15\n19.58\n20.87\n▂▇▁▅▇\n\n\ntreat2\n0\n1\n16.89\n2.31\n13.39\n14.94\n17.33\n18.06\n20.25\n▇▇▃▇▇\n\n\ntreat3\n0\n1\n12.44\n0.85\n11.79\n11.93\n12.33\n12.42\n14.58\n▇▅▁▁▂\n\n\n\n\n\n##Look at the data We could do this in the wide format but it is a lot easier in long format ## Wide to long format\n\n# this will add an index to the dataframe so you know what individual is which\ngene_exp_long.df &lt;- gene_exp.df %&gt;%\n  gather(\n    treatment, # this will take all the row headings into a column\n    expression, # this will convert all the measures into a column called expression\n    -replicate, # the - sign tells tidyr not to move those columns\n    -individual\n  ) \n\nglimpse(gene_exp_long.df)\n\nRows: 36\nColumns: 4\n$ individual &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2,…\n$ replicate  &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1,…\n$ treatment  &lt;chr&gt; \"control\", \"control\", \"control\", \"control\", \"control\", \"con…\n$ expression &lt;dbl&gt; 12.59708, 12.48503, 12.04311, 12.46447, 12.02386, 12.65620,…\n\n\n\n\n\n\nglimpse(gene_exp_long.df)\n\nRows: 36\nColumns: 4\n$ individual &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2,…\n$ replicate  &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1,…\n$ treatment  &lt;chr&gt; \"control\", \"control\", \"control\", \"control\", \"control\", \"con…\n$ expression &lt;dbl&gt; 12.59708, 12.48503, 12.04311, 12.46447, 12.02386, 12.65620,…\n\n\n##Convert to factors So to do this the experiment and individuals are numeric and need to be converted to categories or factors…\n\n\n\n\n# convert things to factors or do calculations\ngene_exp.df &lt;- gene_exp.df %&gt;%\n  mutate(\n    replicate = as.factor(replicate),\n    individual = as.factor(individual)\n  )\n\n# now look at it\nskim(gene_exp.df)\n\n\nData summary\n\n\nName\ngene_exp.df\n\n\nNumber of rows\n9\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nindividual\n0\n1\nFALSE\n9\n1: 1, 2: 1, 3: 1, 4: 1\n\n\nreplicate\n0\n1\nFALSE\n3\n1: 3, 2: 3, 3: 3\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncontrol\n0\n1\n12.37\n0.24\n12.02\n12.16\n12.46\n12.53\n12.66\n▅▂▂▅▇\n\n\ntreat1\n0\n1\n17.31\n2.90\n12.27\n15.67\n18.15\n19.58\n20.87\n▂▇▁▅▇\n\n\ntreat2\n0\n1\n16.89\n2.31\n13.39\n14.94\n17.33\n18.06\n20.25\n▇▇▃▇▇\n\n\ntreat3\n0\n1\n12.44\n0.85\n11.79\n11.93\n12.33\n12.42\n14.58\n▇▅▁▁▂\n\n\n\n\n\n\n\n\n\n#need to make treatment a factor\ngene_exp_long.df &lt;- gene_exp_long.df %&gt;%\nmutate(\n  replicate = as.factor(replicate),\n    individual = as.factor(individual),\n    treatment = as.factor(treatment)\n  )\n\n# now look at it\nskim(gene_exp_long.df)\n\n\nData summary\n\n\nName\ngene_exp_long.df\n\n\nNumber of rows\n36\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nindividual\n0\n1\nFALSE\n9\n1: 4, 2: 4, 3: 4, 4: 4\n\n\nreplicate\n0\n1\nFALSE\n3\n1: 12, 2: 12, 3: 12\n\n\ntreatment\n0\n1\nFALSE\n4\ncon: 9, tre: 9, tre: 9, tre: 9\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nexpression\n0\n1\n14.75\n3\n11.79\n12.34\n13.02\n17.45\n20.87\n▇▂▁▂▂\n\n\n\n\n\n##Now to graph the long format data\n\n# Note the new format allows us to make coding a lot faster\ngene_exp_long.df %&gt;% \n  group_by(treatment)  %&gt;%\n  ggplot(aes(treatment, expression)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Great coverage of this material is Dolph Schluters page https://www.zoology.ubc.ca/~schluter/R/fit-model/\nThe key thing here is the use of the car package as it is essential for unbalanced designs and the use of Type III sum of squares otherwise Type I sum of squares are used which is rarely good. Weather to use Type II or III is a contentious issue and we will just go with Type III for all of our work\n\n\n\n\n\n#Compared variances uisng  Bartlet test\n# significant means they differ in varainces but this is close\nbartlett.test(expression ~ treatment, data=gene_exp_long.df)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  expression by treatment\nBartlett's K-squared = 34.668, df = 3, p-value = 1.432e-07\n\n\n\n\n\nNow we can run an ANOVA as long as the categories we are testing are factors. When doing this we will test to see if any of the means are different but will not be able to tell what is different yet. Thatis the next step\n\n# ANOVA - ONE WAY\n# you can do an anova as an anova and not the linear model\n#Run the anova and store it in the model in Values\nexpression.model.aov = aov(expression ~ treatment, data=gene_exp_long.df)\n\n#Obtain the anova table\nanova(expression.model.aov)\n\nAnalysis of Variance Table\n\nResponse: expression\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ntreatment  3 199.26  66.421  18.334 4.189e-07 ***\nResiduals 32 115.93   3.623                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n# save model to a text file for excel or whatever\ntidy(expression.model.aov)\n\n# A tibble: 2 × 6\n  term         df sumsq meansq statistic      p.value\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 treatment     3  199.  66.4       18.3  0.000000419\n2 Residuals    32  116.   3.62      NA   NA          \n\n#You can copy this out or save it as an object and then save it as a csv file\n# save the model\n# tidy_expression.model.aov &lt;- tidy(expression.model.aov)\n# write_csv(expression.model.aov, \"tidy_anova_expression.csv\")\n\n\n\n\n\n# Plot residuals\n#Base R plots\nplot(fitted(expression.model.aov), residuals(expression.model.aov))\n\n\n\n\n\n\n\n\n\n\n\n\n#Histogram of residuals\nhist(residuals(expression.model.aov), \n     col=\"darkgray\")\n\n\n\n\n\n\n\n\n\n\n\n\n# check for normally distributed data\nqqnorm(expression.model.aov$res)\n\n\n\n\n\n\n\n\n\n\n\n\n#Test for normality of residuals\nshapiro.test(expression.model.aov$res)\n\n\n    Shapiro-Wilk normality test\n\ndata:  expression.model.aov$res\nW = 0.9572, p-value = 0.176\n\n\n\n\n\n\n# Post F tests\n# Comparisons of species\nlsm = emmeans(expression.model.aov, \n              \"treatment\",\n              adjust=\"bonferroni\")\n\n### Means sharing a letter in .group are not significantly different\n#Note that this requires multcompView\nmultcomp::cld(lsm,\n    alpha=.05,\n    Letters=letters)\n\n treatment emmean    SE df lower.CL upper.CL .group\n control     12.4 0.634 32     10.7     14.0  a    \n treat3      12.4 0.634 32     10.8     14.1  a    \n treat2      16.9 0.634 32     15.2     18.6   b   \n treat1      17.3 0.634 32     15.6     19.0   b   \n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 4 estimates \nP value adjustment: tukey method for comparing a family of 4 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\n\n# Now you have a statistical test of how the means compare\ngene_exp_long.df %&gt;% \n  group_by(treatment)  %&gt;%\n  ggplot(aes(treatment, expression)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n# what if you wanted a bar plot\nggplot(gene_exp_long.df, aes(x= treatment))+\n  stat_summary(aes(y=expression), fun.y=mean, geom='bar', color=\"black\", \n               fill=\"blue\", alpha=0.5) +\n  stat_summary(aes(y=expression), fun.data = mean_se, geom = \"errorbar\", color=\"black\", width=0.2)\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead."
  },
  {
    "objectID": "17_one_way_anova.html#read-in-the-file",
    "href": "17_one_way_anova.html#read-in-the-file",
    "title": "One Way ANOVA",
    "section": "",
    "text": "# read in the file\ngene_exp.df &lt;- read_csv(\"data/gene_data.csv\") %&gt;%\n  clean_names() \n\nRows: 9 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): individual, replicate, control, treat1, treat2, treat3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(gene_exp.df)\n\nRows: 9\nColumns: 6\n$ individual &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9\n$ replicate  &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3\n$ control    &lt;dbl&gt; 12.59708, 12.48503, 12.04311, 12.46447, 12.02386, 12.65620,…\n$ treat1     &lt;dbl&gt; 20.87269, 19.57602, 15.66664, 15.70462, 20.20846, 18.15278,…\n$ treat2     &lt;dbl&gt; 14.94204, 15.90944, 13.39281, 17.80308, 20.25343, 19.59284,…\n$ treat3     &lt;dbl&gt; 11.90506, 12.42411, 12.34661, 11.93034, 11.78713, 12.07246,…\n\n\n##Summary Statistics for the better look\n\n# the data you want to look at\nskim(gene_exp.df)\n\n\nData summary\n\n\nName\ngene_exp.df\n\n\nNumber of rows\n9\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nindividual\n0\n1\n5.00\n2.74\n1.00\n3.00\n5.00\n7.00\n9.00\n▇▇▃▇▇\n\n\nreplicate\n0\n1\n2.00\n0.87\n1.00\n1.00\n2.00\n3.00\n3.00\n▇▁▇▁▇\n\n\ncontrol\n0\n1\n12.37\n0.24\n12.02\n12.16\n12.46\n12.53\n12.66\n▅▂▂▅▇\n\n\ntreat1\n0\n1\n17.31\n2.90\n12.27\n15.67\n18.15\n19.58\n20.87\n▂▇▁▅▇\n\n\ntreat2\n0\n1\n16.89\n2.31\n13.39\n14.94\n17.33\n18.06\n20.25\n▇▇▃▇▇\n\n\ntreat3\n0\n1\n12.44\n0.85\n11.79\n11.93\n12.33\n12.42\n14.58\n▇▅▁▁▂\n\n\n\n\n\n##Look at the data We could do this in the wide format but it is a lot easier in long format ## Wide to long format\n\n# this will add an index to the dataframe so you know what individual is which\ngene_exp_long.df &lt;- gene_exp.df %&gt;%\n  gather(\n    treatment, # this will take all the row headings into a column\n    expression, # this will convert all the measures into a column called expression\n    -replicate, # the - sign tells tidyr not to move those columns\n    -individual\n  ) \n\nglimpse(gene_exp_long.df)\n\nRows: 36\nColumns: 4\n$ individual &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2,…\n$ replicate  &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1,…\n$ treatment  &lt;chr&gt; \"control\", \"control\", \"control\", \"control\", \"control\", \"con…\n$ expression &lt;dbl&gt; 12.59708, 12.48503, 12.04311, 12.46447, 12.02386, 12.65620,…"
  },
  {
    "objectID": "17_one_way_anova.html#look-at-data",
    "href": "17_one_way_anova.html#look-at-data",
    "title": "One Way ANOVA",
    "section": "",
    "text": "glimpse(gene_exp_long.df)\n\nRows: 36\nColumns: 4\n$ individual &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2,…\n$ replicate  &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1,…\n$ treatment  &lt;chr&gt; \"control\", \"control\", \"control\", \"control\", \"control\", \"con…\n$ expression &lt;dbl&gt; 12.59708, 12.48503, 12.04311, 12.46447, 12.02386, 12.65620,…\n\n\n##Convert to factors So to do this the experiment and individuals are numeric and need to be converted to categories or factors…"
  },
  {
    "objectID": "17_one_way_anova.html#wide-data-to-factors",
    "href": "17_one_way_anova.html#wide-data-to-factors",
    "title": "One Way ANOVA",
    "section": "",
    "text": "# convert things to factors or do calculations\ngene_exp.df &lt;- gene_exp.df %&gt;%\n  mutate(\n    replicate = as.factor(replicate),\n    individual = as.factor(individual)\n  )\n\n# now look at it\nskim(gene_exp.df)\n\n\nData summary\n\n\nName\ngene_exp.df\n\n\nNumber of rows\n9\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nindividual\n0\n1\nFALSE\n9\n1: 1, 2: 1, 3: 1, 4: 1\n\n\nreplicate\n0\n1\nFALSE\n3\n1: 3, 2: 3, 3: 3\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncontrol\n0\n1\n12.37\n0.24\n12.02\n12.16\n12.46\n12.53\n12.66\n▅▂▂▅▇\n\n\ntreat1\n0\n1\n17.31\n2.90\n12.27\n15.67\n18.15\n19.58\n20.87\n▂▇▁▅▇\n\n\ntreat2\n0\n1\n16.89\n2.31\n13.39\n14.94\n17.33\n18.06\n20.25\n▇▇▃▇▇\n\n\ntreat3\n0\n1\n12.44\n0.85\n11.79\n11.93\n12.33\n12.42\n14.58\n▇▅▁▁▂"
  },
  {
    "objectID": "17_one_way_anova.html#long-data-to-factors",
    "href": "17_one_way_anova.html#long-data-to-factors",
    "title": "One Way ANOVA",
    "section": "",
    "text": "#need to make treatment a factor\ngene_exp_long.df &lt;- gene_exp_long.df %&gt;%\nmutate(\n  replicate = as.factor(replicate),\n    individual = as.factor(individual),\n    treatment = as.factor(treatment)\n  )\n\n# now look at it\nskim(gene_exp_long.df)\n\n\nData summary\n\n\nName\ngene_exp_long.df\n\n\nNumber of rows\n36\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nindividual\n0\n1\nFALSE\n9\n1: 4, 2: 4, 3: 4, 4: 4\n\n\nreplicate\n0\n1\nFALSE\n3\n1: 12, 2: 12, 3: 12\n\n\ntreatment\n0\n1\nFALSE\n4\ncon: 9, tre: 9, tre: 9, tre: 9\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nexpression\n0\n1\n14.75\n3\n11.79\n12.34\n13.02\n17.45\n20.87\n▇▂▁▂▂\n\n\n\n\n\n##Now to graph the long format data\n\n# Note the new format allows us to make coding a lot faster\ngene_exp_long.df %&gt;% \n  group_by(treatment)  %&gt;%\n  ggplot(aes(treatment, expression)) +\n  geom_boxplot()"
  },
  {
    "objectID": "17_one_way_anova.html#one-way-anova",
    "href": "17_one_way_anova.html#one-way-anova",
    "title": "One Way ANOVA",
    "section": "",
    "text": "A Great coverage of this material is Dolph Schluters page https://www.zoology.ubc.ca/~schluter/R/fit-model/\nThe key thing here is the use of the car package as it is essential for unbalanced designs and the use of Type III sum of squares otherwise Type I sum of squares are used which is rarely good. Weather to use Type II or III is a contentious issue and we will just go with Type III for all of our work"
  },
  {
    "objectID": "17_one_way_anova.html#bartlets-test-for-homogeneity-of-varaince",
    "href": "17_one_way_anova.html#bartlets-test-for-homogeneity-of-varaince",
    "title": "One Way ANOVA",
    "section": "",
    "text": "#Compared variances uisng  Bartlet test\n# significant means they differ in varainces but this is close\nbartlett.test(expression ~ treatment, data=gene_exp_long.df)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  expression by treatment\nBartlett's K-squared = 34.668, df = 3, p-value = 1.432e-07"
  },
  {
    "objectID": "17_one_way_anova.html#anova",
    "href": "17_one_way_anova.html#anova",
    "title": "One Way ANOVA",
    "section": "",
    "text": "Now we can run an ANOVA as long as the categories we are testing are factors. When doing this we will test to see if any of the means are different but will not be able to tell what is different yet. Thatis the next step\n\n# ANOVA - ONE WAY\n# you can do an anova as an anova and not the linear model\n#Run the anova and store it in the model in Values\nexpression.model.aov = aov(expression ~ treatment, data=gene_exp_long.df)\n\n#Obtain the anova table\nanova(expression.model.aov)\n\nAnalysis of Variance Table\n\nResponse: expression\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ntreatment  3 199.26  66.421  18.334 4.189e-07 ***\nResiduals 32 115.93   3.623                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "17_one_way_anova.html#save-the-model-for-use-in-word",
    "href": "17_one_way_anova.html#save-the-model-for-use-in-word",
    "title": "One Way ANOVA",
    "section": "",
    "text": "# save model to a text file for excel or whatever\ntidy(expression.model.aov)\n\n# A tibble: 2 × 6\n  term         df sumsq meansq statistic      p.value\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 treatment     3  199.  66.4       18.3  0.000000419\n2 Residuals    32  116.   3.62      NA   NA          \n\n#You can copy this out or save it as an object and then save it as a csv file\n# save the model\n# tidy_expression.model.aov &lt;- tidy(expression.model.aov)\n# write_csv(expression.model.aov, \"tidy_anova_expression.csv\")"
  },
  {
    "objectID": "17_one_way_anova.html#plot-residuals",
    "href": "17_one_way_anova.html#plot-residuals",
    "title": "One Way ANOVA",
    "section": "",
    "text": "# Plot residuals\n#Base R plots\nplot(fitted(expression.model.aov), residuals(expression.model.aov))"
  },
  {
    "objectID": "17_one_way_anova.html#histogram-of-residuals",
    "href": "17_one_way_anova.html#histogram-of-residuals",
    "title": "One Way ANOVA",
    "section": "",
    "text": "#Histogram of residuals\nhist(residuals(expression.model.aov), \n     col=\"darkgray\")"
  },
  {
    "objectID": "17_one_way_anova.html#check-for-normality",
    "href": "17_one_way_anova.html#check-for-normality",
    "title": "One Way ANOVA",
    "section": "",
    "text": "# check for normally distributed data\nqqnorm(expression.model.aov$res)"
  },
  {
    "objectID": "17_one_way_anova.html#statistical-test-of-normality",
    "href": "17_one_way_anova.html#statistical-test-of-normality",
    "title": "One Way ANOVA",
    "section": "",
    "text": "#Test for normality of residuals\nshapiro.test(expression.model.aov$res)\n\n\n    Shapiro-Wilk normality test\n\ndata:  expression.model.aov$res\nW = 0.9572, p-value = 0.176"
  },
  {
    "objectID": "17_one_way_anova.html#post-f-tests-of-an-anova",
    "href": "17_one_way_anova.html#post-f-tests-of-an-anova",
    "title": "One Way ANOVA",
    "section": "",
    "text": "# Post F tests\n# Comparisons of species\nlsm = emmeans(expression.model.aov, \n              \"treatment\",\n              adjust=\"bonferroni\")\n\n### Means sharing a letter in .group are not significantly different\n#Note that this requires multcompView\nmultcomp::cld(lsm,\n    alpha=.05,\n    Letters=letters)\n\n treatment emmean    SE df lower.CL upper.CL .group\n control     12.4 0.634 32     10.7     14.0  a    \n treat3      12.4 0.634 32     10.8     14.1  a    \n treat2      16.9 0.634 32     15.2     18.6   b   \n treat1      17.3 0.634 32     15.6     19.0   b   \n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 4 estimates \nP value adjustment: tukey method for comparing a family of 4 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\n\n# Now you have a statistical test of how the means compare\ngene_exp_long.df %&gt;% \n  group_by(treatment)  %&gt;%\n  ggplot(aes(treatment, expression)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n# what if you wanted a bar plot\nggplot(gene_exp_long.df, aes(x= treatment))+\n  stat_summary(aes(y=expression), fun.y=mean, geom='bar', color=\"black\", \n               fill=\"blue\", alpha=0.5) +\n  stat_summary(aes(y=expression), fun.data = mean_se, geom = \"errorbar\", color=\"black\", width=0.2)\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead."
  },
  {
    "objectID": "18_two_way_anova.html",
    "href": "18_two_way_anova.html",
    "title": "Two Way ANOVA",
    "section": "",
    "text": "Objective\nThe goal of this page is to learn to perform an two way ANOVA\n##Load libraries We will read in the main files and load the libraries as we have worked with so far.\n\n# #Install Packages ----\n# install.packages(\"tidyverse\")\n# install.packages(\"lubridate\")\n# install.packages(\"scales\")\n# install.packages(\"readxl\")\n# install.packages(\"survminer\")\n# install.packages(\"survival\")\n# install.packages(\"patchwork\")\n# install.packages(\"broom\")\n# ANOVA specific\n# install.packages(\"car\")\n# install.packages(\"emmeans\")\n# install.packages(\"multcompView\")\n\n\n#Load libraries ----\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(readxl)\nlibrary(skimr)\nlibrary(broom)\nlibrary(janitor)\n# library(zoo)\nlibrary(patchwork)\nlibrary(car)\nlibrary(emmeans)\nlibrary(multcompView)\n\nThe data for this project was downloaded from http://stat.pugetsound.edu/hoard/datasetDetails.aspx?id=1\nThis is a super fun data set in my opinion on m&m’s\n\n# read file----\nmm.df &lt;- read_csv(\"data/mms.csv\")\n\n\nmm.df %&gt;% \n  ggplot(aes(center, color=color)) +\n  stat_summary(aes(y = mass ),\n               fun.y = mean,\n               geom = \"point\",\n               size = 4,\n               position = position_dodge(0.3),\n               na.rm = TRUE) +\n  stat_summary(aes(y = mass),\n               fun.data = mean_se,\n               geom = \"errorbar\",\n               width = 0.2,\n               position = position_dodge(0.3),\n               na.rm = TRUE) \n\n\n\n\n\n\n\n\n\nmm.df &lt;- mm.df %&gt;%\n  mutate(\n    center = as.factor(center),\n    color = as.factor(color)\n  )\n\n\nmm.df &lt;- mm.df %&gt;%\n  mutate(\n    center = fct_relevel(center, \n                         \"plain\", \"peanut butter\", \"peanut\"),\n    color = fct_relevel(color, \n                        \"red\", \"blue\", \"brown\", \"green\", \"orange\", \"yellow\" )\n  )\n\n\n# Set it up for Type III SS ANOVA\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\n\n\n# Fit the linear model and conduct ANOVA\nmodel = lm(mass ~ center*color, data=mm.df)\nAnova(model, type=\"III\")       # Use type=\"III\" ALWAYS!!!!\n\nAnova Table (Type III tests)\n\nResponse: mass\n              Sum Sq  Df    F value    Pr(&gt;F)    \n(Intercept)  1928.03   1 48402.2077 &lt; 2.2e-16 ***\ncenter        373.84   2  4692.5209 &lt; 2.2e-16 ***\ncolor           0.81   5     4.0567  0.001214 ** \ncenter:color    0.77  10     1.9239  0.038945 *  \nResiduals      31.79 798                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# Post F test of interactions -----\nmodel.emm &lt;- emmeans(model, ~ center * color)\n\n\n# plot of comparisons\n# blue are confidence intervals, red arrows overlap mean no significant diff\nplot(model.emm, comparisons = TRUE)\n\n\n\n\n\n\n\n\n\n# pairwise\nemminteraction = emmeans(model, \n                         pairwise ~ center:color,\n                         adjust=\"bonferroni\")\n\nemminteraction$contrasts\n\n contrast                                     estimate     SE  df t.ratio\n plain red - peanut butter red               -0.885993 0.0508 798 -17.431\n plain red - peanut red                      -1.772017 0.0518 798 -34.239\n plain red - plain blue                      -0.005747 0.0338 798  -0.170\n plain red - peanut butter blue              -0.998017 0.0459 798 -21.730\n plain red - peanut blue                     -1.721443 0.0465 798 -37.022\n plain red - plain brown                     -0.016099 0.0339 798  -0.475\n plain red - peanut butter brown             -0.948612 0.0404 798 -23.459\n plain red - peanut brown                    -1.716822 0.0492 798 -34.909\n plain red - plain green                     -0.015409 0.0335 798  -0.460\n plain red - peanut butter green             -1.065811 0.0431 798 -24.724\n plain red - peanut green                    -1.826258 0.0465 798 -39.276\n plain red - plain orange                    -0.010317 0.0349 798  -0.296\n plain red - peanut butter orange            -0.875517 0.0484 798 -18.074\n plain red - peanut orange                   -1.715862 0.0454 798 -37.802\n plain red - plain yellow                    -0.010986 0.0362 798  -0.304\n plain red - peanut butter yellow            -0.885133 0.0381 798 -23.222\n plain red - peanut yellow                   -1.712554 0.0465 798 -36.830\n peanut butter red - peanut red              -0.886024 0.0624 798 -14.209\n peanut butter red - plain blue               0.880246 0.0485 798  18.140\n peanut butter red - peanut butter blue      -0.112024 0.0576 798  -1.944\n peanut butter red - peanut blue             -0.835450 0.0581 798 -14.387\n peanut butter red - plain brown              0.869895 0.0486 798  17.906\n peanut butter red - peanut butter brown     -0.062619 0.0533 798  -1.174\n peanut butter red - peanut brown            -0.830828 0.0602 798 -13.792\n peanut butter red - plain green              0.870585 0.0483 798  18.036\n peanut butter red - peanut butter green     -0.179818 0.0554 798  -3.246\n peanut butter red - peanut green            -0.940265 0.0581 798 -16.192\n peanut butter red - plain orange             0.875676 0.0493 798  17.772\n peanut butter red - peanut butter orange     0.010476 0.0596 798   0.176\n peanut butter red - peanut orange           -0.829869 0.0572 798 -14.511\n peanut butter red - plain yellow             0.875007 0.0502 798  17.433\n peanut butter red - peanut butter yellow     0.000861 0.0516 798   0.017\n peanut butter red - peanut yellow           -0.826561 0.0581 798 -14.234\n peanut red - plain blue                      1.766270 0.0495 798  35.687\n peanut red - peanut butter blue              0.774000 0.0584 798  13.246\n peanut red - peanut blue                     0.050574 0.0589 798   0.859\n peanut red - plain brown                     1.755919 0.0495 798  35.440\n peanut red - peanut butter brown             0.823405 0.0542 798  15.186\n peanut red - peanut brown                    0.055196 0.0610 798   0.905\n peanut red - plain green                     1.756609 0.0492 798  35.674\n peanut red - peanut butter green             0.706206 0.0562 798  12.556\n peanut red - peanut green                   -0.054241 0.0589 798  -0.921\n peanut red - plain orange                    1.761700 0.0502 798  35.075\n peanut red - peanut butter orange            0.896500 0.0604 798  14.836\n peanut red - peanut orange                   0.056155 0.0580 798   0.968\n peanut red - plain yellow                    1.761031 0.0511 798  34.444\n peanut red - peanut butter yellow            0.886885 0.0525 798  16.889\n peanut red - peanut yellow                   0.059463 0.0589 798   1.010\n plain blue - peanut butter blue             -0.992270 0.0434 798 -22.882\n plain blue - peanut blue                    -1.715696 0.0440 798 -39.022\n plain blue - plain brown                    -0.010351 0.0303 798  -0.341\n plain blue - peanut butter brown            -0.942865 0.0375 798 -25.143\n plain blue - peanut brown                   -1.711075 0.0468 798 -36.566\n plain blue - plain green                    -0.009661 0.0298 798  -0.324\n plain blue - peanut butter green            -1.060064 0.0404 798 -26.261\n plain blue - peanut green                   -1.820511 0.0440 798 -41.406\n plain blue - plain orange                   -0.004570 0.0314 798  -0.145\n plain blue - peanut butter orange           -0.869770 0.0460 798 -18.901\n plain blue - peanut orange                  -1.710115 0.0428 798 -39.961\n plain blue - plain yellow                   -0.005239 0.0329 798  -0.159\n plain blue - peanut butter yellow           -0.879386 0.0350 798 -25.137\n plain blue - peanut yellow                  -1.706807 0.0440 798 -38.819\n peanut butter blue - peanut blue            -0.723426 0.0538 798 -13.438\n peanut butter blue - plain brown             0.981919 0.0434 798  22.611\n peanut butter blue - peanut butter brown     0.049405 0.0487 798   1.015\n peanut butter blue - peanut brown           -0.718804 0.0562 798 -12.798\n peanut butter blue - plain green             0.982609 0.0431 798  22.811\n peanut butter blue - peanut butter green    -0.067794 0.0509 798  -1.331\n peanut butter blue - peanut green           -0.828241 0.0538 798 -15.386\n peanut butter blue - plain orange            0.987700 0.0442 798  22.346\n peanut butter blue - peanut butter orange    0.122500 0.0555 798   2.206\n peanut butter blue - peanut orange          -0.717845 0.0529 798 -13.575\n peanut butter blue - plain yellow            0.987031 0.0452 798  21.826\n peanut butter blue - peanut butter yellow    0.112885 0.0468 798   2.413\n peanut butter blue - peanut yellow          -0.714537 0.0538 798 -13.273\n peanut blue - plain brown                    1.705345 0.0440 798  38.733\n peanut blue - peanut butter brown            0.772831 0.0492 798  15.698\n peanut blue - peanut brown                   0.004622 0.0566 798   0.082\n peanut blue - plain green                    1.706035 0.0437 798  39.054\n peanut blue - peanut butter green            0.655632 0.0514 798  12.744\n peanut blue - peanut green                  -0.104815 0.0543 798  -1.930\n peanut blue - plain orange                   1.711126 0.0448 798  38.201\n peanut blue - peanut butter orange           0.845926 0.0560 798  15.108\n peanut blue - peanut orange                  0.005581 0.0534 798   0.105\n peanut blue - plain yellow                   1.710457 0.0458 798  37.346\n peanut blue - peanut butter yellow           0.836310 0.0473 798  17.665\n peanut blue - peanut yellow                  0.008889 0.0543 798   0.164\n plain brown - peanut butter brown           -0.932514 0.0376 798 -24.820\n plain brown - peanut brown                  -1.700723 0.0469 798 -36.300\n plain brown - plain green                    0.000690 0.0299 798   0.023\n plain brown - peanut butter green           -1.049713 0.0404 798 -25.962\n plain brown - peanut green                  -1.810159 0.0440 798 -41.114\n plain brown - plain orange                   0.005781 0.0315 798   0.183\n plain brown - peanut butter orange          -0.859419 0.0461 798 -18.653\n plain brown - peanut orange                 -1.699763 0.0429 798 -39.661\n plain brown - plain yellow                   0.005113 0.0329 798   0.155\n plain brown - peanut butter yellow          -0.869034 0.0351 798 -24.787\n plain brown - peanut yellow                 -1.696456 0.0440 798 -38.531\n peanut butter brown - peanut brown          -0.768209 0.0518 798 -14.838\n peanut butter brown - plain green            0.933204 0.0372 798  25.108\n peanut butter brown - peanut butter green   -0.117199 0.0460 798  -2.545\n peanut butter brown - peanut green          -0.877645 0.0492 798 -17.827\n peanut butter brown - plain orange           0.938295 0.0385 798  24.394\n peanut butter brown - peanut butter orange   0.073095 0.0511 798   1.431\n peanut butter brown - peanut orange         -0.767250 0.0482 798 -15.922\n peanut butter brown - plain yellow           0.937627 0.0396 798  23.657\n peanut butter brown - peanut butter yellow   0.063480 0.0414 798   1.533\n peanut butter brown - peanut yellow         -0.763942 0.0492 798 -15.517\n peanut brown - plain green                   1.701413 0.0465 798  36.567\n peanut brown - peanut butter green           0.651010 0.0539 798  12.082\n peanut brown - peanut green                 -0.109436 0.0566 798  -1.932\n peanut brown - plain orange                  1.706504 0.0476 798  35.873\n peanut brown - peanut butter orange          0.841304 0.0582 798  14.446\n peanut brown - peanut orange                 0.000959 0.0557 798   0.017\n peanut brown - plain yellow                  1.705836 0.0485 798  35.157\n peanut brown - peanut butter yellow          0.831689 0.0500 798  16.641\n peanut brown - peanut yellow                 0.004267 0.0566 798   0.075\n plain green - peanut butter green           -1.050403 0.0401 798 -26.223\n plain green - peanut green                  -1.810849 0.0437 798 -41.453\n plain green - plain orange                   0.005091 0.0310 798   0.164\n plain green - peanut butter orange          -0.860109 0.0457 798 -18.802\n plain green - peanut orange                 -1.700454 0.0425 798 -40.007\n plain green - plain yellow                   0.004423 0.0325 798   0.136\n plain green - peanut butter yellow          -0.869724 0.0346 798 -25.117\n plain green - peanut yellow                 -1.697146 0.0437 798 -38.851\n peanut butter green - peanut green          -0.760447 0.0514 798 -14.781\n peanut butter green - plain orange           1.055494 0.0413 798  25.579\n peanut butter green - peanut butter orange   0.190294 0.0532 798   3.576\n peanut butter green - peanut orange         -0.650051 0.0504 798 -12.885\n peanut butter green - plain yellow           1.054825 0.0424 798  24.904\n peanut butter green - peanut butter yellow   0.180679 0.0440 798   4.105\n peanut butter green - peanut yellow         -0.646743 0.0514 798 -12.571\n peanut green - plain orange                  1.815941 0.0448 798  40.541\n peanut green - peanut butter orange          0.950741 0.0560 798  16.980\n peanut green - peanut orange                 0.110396 0.0534 798   2.068\n peanut green - plain yellow                  1.815272 0.0458 798  39.634\n peanut green - peanut butter yellow          0.941125 0.0473 798  19.879\n peanut green - peanut yellow                 0.113704 0.0543 798   2.093\n plain orange - peanut butter orange         -0.865200 0.0468 798 -18.485\n plain orange - peanut orange                -1.705545 0.0436 798 -39.080\n plain orange - plain yellow                 -0.000669 0.0340 798  -0.020\n plain orange - peanut butter yellow         -0.874815 0.0360 798 -24.290\n plain orange - peanut yellow                -1.702237 0.0448 798 -38.002\n peanut butter orange - peanut orange        -0.840345 0.0551 798 -15.258\n peanut butter orange - plain yellow          0.864531 0.0478 798  18.097\n peanut butter orange - peanut butter yellow -0.009615 0.0493 798  -0.195\n peanut butter orange - peanut yellow        -0.837037 0.0560 798 -14.949\n peanut orange - plain yellow                 1.704876 0.0447 798  38.161\n peanut orange - peanut butter yellow         0.830729 0.0463 798  17.959\n peanut orange - peanut yellow                0.003308 0.0534 798   0.062\n plain yellow - peanut butter yellow         -0.874147 0.0373 798 -23.460\n plain yellow - peanut yellow                -1.701568 0.0458 798 -37.152\n peanut butter yellow - peanut yellow        -0.827422 0.0473 798 -17.477\n p.value\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  0.1864\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  &lt;.0001\n  &lt;.0001\n  0.0565\n  &lt;.0001\n  &lt;.0001\n  0.0068\n  &lt;.0001\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  &lt;.0001\n  1.0000\n  &lt;.0001\n  &lt;.0001\n  &lt;.0001\n\nP value adjustment: bonferroni method for 153 tests \n\n\n\n# # CLD \nmultcomp::cld(model.emm,\n    Letters = letters,\n    adjust=\"bonferroni\") # can be bonferroni- as well\n\n center        color  emmean     SE  df lower.CL upper.CL .group\n plain         red     0.854 0.0262 798    0.776    0.933  a    \n plain         blue    0.860 0.0214 798    0.796    0.924  a    \n plain         orange  0.865 0.0230 798    0.796    0.934  a    \n plain         yellow  0.865 0.0249 798    0.791    0.940  a    \n plain         green   0.870 0.0208 798    0.807    0.932  a    \n plain         brown   0.871 0.0215 798    0.806    0.935  a    \n peanut butter orange  1.730 0.0407 798    1.608    1.852   bc  \n peanut butter yellow  1.740 0.0277 798    1.657    1.823   b   \n peanut butter red     1.740 0.0436 798    1.610    1.871   bc  \n peanut butter brown   1.803 0.0308 798    1.711    1.896   bc  \n peanut butter blue    1.853 0.0377 798    1.739    1.966   bc  \n peanut butter green   1.920 0.0342 798    1.818    2.023    c  \n peanut        yellow  2.567 0.0384 798    2.452    2.682     d \n peanut        orange  2.570 0.0371 798    2.459    2.682     d \n peanut        brown   2.571 0.0416 798    2.446    2.696     d \n peanut        blue    2.576 0.0384 798    2.461    2.691     d \n peanut        red     2.627 0.0446 798    2.493    2.760     d \n peanut        green   2.681 0.0384 798    2.565    2.796     d \n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 18 estimates \nP value adjustment: bonferroni method for 153 tests \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\n\n# assumptions ----\n# Homogeneity of variance----\n# 1. Homogeneity of variances-----\nplot(model, 1)\n\n\n\n\n\n\n\n# Levene test homogeneity of variance ----\nleveneTest(mass ~ center*color, data=mm.df)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(&gt;F)    \ngroup  17  28.574 &lt; 2.2e-16 ***\n      798                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n#Normality of residuals------\nplot(model, 2)\n\n\n\n\n\n\n\n\n\n# Normality of resituals qqnorm-----\nqqnorm(model$res)\n\n\n\n\n\n\n\n\n\n# Shapiro test ----\nshapiro.test(model$res)\n\n\n    Shapiro-Wilk normality test\n\ndata:  model$res\nW = 0.87257, p-value &lt; 2.2e-16"
  }
]